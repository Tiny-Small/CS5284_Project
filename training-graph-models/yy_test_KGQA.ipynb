{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 23826,
     "status": "ok",
     "timestamp": 1729821734370,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "-8ezq4mAhjbh",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4cfe62cf-cc7e-4284-e6a6-307807111490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.0+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
      "Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-3.2.1\n",
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.10)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->torch_geometric) (0.2.0)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers\n",
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12234,
     "status": "ok",
     "timestamp": 1729821888461,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "x8-A-Wy4hJDA",
    "outputId": "3b572d9a-2320-40f1-c80f-d1792962f73e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1729823502709,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "phqpH9Xgh_OJ",
    "outputId": "3403164f-72b1-4fb3-bae0-1520a22ba0d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/CS5284\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/MyDrive/CS5284"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySijWBPZkQS6"
   },
   "source": [
    "### Test training GNN\n",
    "1. Concatenate the question embedding with each node embedding. (GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38703,
     "status": "ok",
     "timestamp": 1729823543139,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "WHfoK9vwkT7S",
    "outputId": "d11b19bd-5296-445f-a82f-982fc4e840c9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# adjust this import accordingly to how you call the script\n",
    "from functions import *\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729823543140,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "i2Eq_lfAyj5v"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 883,
     "status": "ok",
     "timestamp": 1729823544018,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "AzAOKp2kiGQK"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, node_dim, question_dim, hidden_dim, output_dim=1):\n",
    "        super(GCN, self).__init__()\n",
    "        # 4 layers, 100 hidden_dim\n",
    "        self.conv1 = GCNConv(node_dim + question_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv4 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, batched_subgraphs):\n",
    "        # concatenate question embeddings with node features for each subgraph along feature dimension\n",
    "        question_emb_expanded = []\n",
    "        for subgraph in batched_subgraphs.to_data_list():\n",
    "            subgraph.x = torch.cat((subgraph.x, subgraph.qn.unsqueeze(0).expand(subgraph.x.size(0), -1)), dim=1)\n",
    "            question_emb_expanded.append(subgraph.x)\n",
    "\n",
    "        batched_subgraphs.x = torch.cat(question_emb_expanded, dim=0)\n",
    "        x, edge_index = batched_subgraphs.x, batched_subgraphs.edge_index\n",
    "\n",
    "        # GCN\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        # Output logits directly for BCEWithLogitsLoss\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "# Binary classification (answer candidate or not)\n",
    "model = GCN(node_dim=64, question_dim=384, hidden_dim=100).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(logits, labels, alpha=0.25, gamma=2.0):\n",
    "    \"\"\"\n",
    "    NOT USED.\n",
    "    logits: Predicted output from the model (after log-softmax).\n",
    "    labels: Ground truth labels (0 or 1).\n",
    "    alpha: Balancing factor for the minority class.\n",
    "    gamma: Focusing parameter for adjusting the rate at which easy examples are down-weighted.\n",
    "    \"\"\"\n",
    "    # Compute cross-entropy loss per example\n",
    "    ce_loss = F.nll_loss(logits, labels, reduction='none')\n",
    "    # Probabilities for each example\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    # Apply the focal loss adjustment\n",
    "    focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n",
    "\n",
    "    # Return the mean loss\n",
    "    return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1729823544018,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "cNf9bQ45khKQ"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(dataloader):\n",
    "    \"\"\"\n",
    "    returns average loss for each epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # loop batches from dataloader\n",
    "    for batched_subgraphs, labels in dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batched_subgraphs = batched_subgraphs.to(device)\n",
    "        labels = [label.to(device) for label in labels]\n",
    "\n",
    "        # forward pass\n",
    "        out = model(batched_subgraphs)\n",
    "\n",
    "        # calculate loss\n",
    "        batch_loss = 0\n",
    "        for i, label in enumerate(labels):\n",
    "            node_mask = (batched_subgraphs.batch == i)\n",
    "            logits = out[node_mask]\n",
    "            target = label.float()\n",
    "\n",
    "            pos_weight = torch.tensor([len(target) / target.sum()], device=device)\n",
    "            loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "            batch_loss += loss_fn(logits, target)\n",
    "\n",
    "        # backward pass and optimization step\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # shift back to cpu\n",
    "        batch_loss = batch_loss.detach().cpu()\n",
    "        output_cpu = out.detach().cpu()\n",
    "        labels_cpu = [label.detach().cpu() for label in labels]\n",
    "\n",
    "        print('Batch loss is', batch_loss.item())\n",
    "\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    torch.cuda.empty_cache() # help clear cache taking up cuda space\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1729823544019,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "m9axFSFO7tkF"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batched_subgraphs, labels in dataloader:\n",
    "            batched_subgraphs = batched_subgraphs.to(device)\n",
    "            labels = [label for label in labels]\n",
    "\n",
    "            out = model(batched_subgraphs)\n",
    "            output_cpu = out.detach().cpu()\n",
    "\n",
    "            # calculate accuracy for each subgraph\n",
    "            for i, label in enumerate(labels):\n",
    "                node_mask = (batched_subgraphs.batch == i).detach().cpu()\n",
    "                preds = (torch.sigmoid(output_cpu[node_mask]) > 0.5).int()\n",
    "                \n",
    "                all_preds.extend(preds.tolist())\n",
    "                all_labels.extend(label.tolist())\n",
    "                correct += (preds == label).sum().item()\n",
    "                total += label.size(0)\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='binary') # for positive class\n",
    "    recall = recall_score(all_labels, all_preds, average='binary') # for positive class\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary') # for positive class\n",
    "    accuracy = correct / total # biased towards 0\n",
    "\n",
    "    torch.cuda.empty_cache() # help clear cache taking up cuda space\n",
    "\n",
    "    return accuracy, precision, recall, f1, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TGKM8P1k5C8z"
   },
   "outputs": [],
   "source": [
    "path_to_node_embed = '../Datasets/MetaQA_dataset/processed/node2vec _embeddings/ud_node2vec_embeddings.txt'\n",
    "path_to_idxes = '../Datasets/MetaQA_dataset/processed/idxes.json'\n",
    "path_to_qa = '../Datasets/MetaQA_dataset/vanilla 3-hop/qa_train.txt'\n",
    "\n",
    "# train\n",
    "data = KGQADataset(path_to_node_embed, path_to_idxes, path_to_qa)\n",
    "sub_data1 = torch.utils.data.Subset(data, list(range(2000)))\n",
    "dataloader_train = DataLoader(sub_data1, batch_size=64, collate_fn=collate_fn, shuffle=True)\n",
    "# some from train to evaluate\n",
    "sub_data2 = torch.utils.data.Subset(data, list(range(5000, 5000+400)))\n",
    "dataloader_val = DataLoader(sub_data2, batch_size=64, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1729823495975,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "NkKweyLT-RTY"
   },
   "outputs": [],
   "source": [
    "# some from test to evaluate\n",
    "test = KGQADataset(path_to_node_embed, path_to_idxes, '../Datasets/MetaQA_dataset/vanilla 3-hop/qa_test.txt')\n",
    "sub_data3 = torch.utils.data.Subset(test, list(range(400)))\n",
    "dataloader_test = DataLoader(sub_data3, batch_size=64, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 248873,
     "status": "ok",
     "timestamp": 1729785041431,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "vwWWDog5xt8J",
    "outputId": "ec06b29d-fe3a-4c8f-cabe-3db0d521d2ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss is 89.95256042480469\n",
      "Batch loss is 93.21959686279297\n",
      "Batch loss is 81.2720718383789\n",
      "Batch loss is 82.74179077148438\n",
      "Batch loss is 81.05941009521484\n",
      "Batch loss is 79.3653335571289\n",
      "Batch loss is 80.254150390625\n",
      "Batch loss is 79.60807800292969\n",
      "Batch loss is 79.48625183105469\n",
      "Batch loss is 79.6988296508789\n",
      "Batch loss is 81.76297760009766\n",
      "Batch loss is 82.50495147705078\n",
      "Batch loss is 78.03034973144531\n",
      "Batch loss is 78.94815826416016\n",
      "Batch loss is 80.22224426269531\n",
      "Batch loss is 79.8222885131836\n",
      "Batch loss is 77.80859375\n",
      "Batch loss is 76.4232177734375\n",
      "Batch loss is 75.96676635742188\n",
      "Batch loss is 77.85187530517578\n",
      "Batch loss is 77.0613021850586\n",
      "Batch loss is 77.3415298461914\n",
      "Batch loss is 75.35107421875\n",
      "Batch loss is 77.83760070800781\n",
      "Batch loss is 72.38226318359375\n",
      "Batch loss is 75.3498764038086\n",
      "Batch loss is 77.80155181884766\n",
      "Batch loss is 79.95442199707031\n",
      "Batch loss is 75.32730102539062\n",
      "Batch loss is 75.8548355102539\n",
      "Batch loss is 74.46247100830078\n",
      "Batch loss is 19.731216430664062\n",
      "Epoch 0, Train Loss: 77.32671689987183\n",
      "Validation Accuracy: 0.39115200, Validation P/R/F1: 0.001/0.666/0.003\n",
      "Test Accuracy: 0.41191290, Test P/R/F1: 0.001/0.638/0.003\n"
     ]
    }
   ],
   "source": [
    "# train 1 epoch\n",
    "for epoch in range(1):\n",
    "    loss = train(dataloader_train)\n",
    "    val_accuracy, val_p, val_r, val_f1, _, _ = evaluate(dataloader_val)\n",
    "    test_accuracy, test_p, test_r, test_f1, _, _ = evaluate(dataloader_test)\n",
    "    print(f'Epoch {epoch}, Train Loss: {loss}')\n",
    "    print(f'Validation Accuracy: {val_accuracy:.8f}, Validation P/R/F1: {val_p:.3f}/{val_r:.3f}/{val_f1:.3f}')\n",
    "    print(f'Test Accuracy: {test_accuracy:.8f}, Test P/R/F1: {test_p:.3f}/{test_r:.3f}/{test_f1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyMZY9ko1UI8o+ma2TA1nv4K",
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
