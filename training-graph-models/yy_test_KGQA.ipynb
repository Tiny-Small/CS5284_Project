{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 23826,
     "status": "ok",
     "timestamp": 1729821734370,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "-8ezq4mAhjbh",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4cfe62cf-cc7e-4284-e6a6-307807111490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.0+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
      "Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-3.2.1\n",
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.10)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->torch_geometric) (0.2.0)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers\n",
    "!pip install torch_geometric\n",
    "!pip install opentsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12234,
     "status": "ok",
     "timestamp": 1729821888461,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "x8-A-Wy4hJDA",
    "outputId": "3b572d9a-2320-40f1-c80f-d1792962f73e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1729823502709,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "phqpH9Xgh_OJ",
    "outputId": "3403164f-72b1-4fb3-bae0-1520a22ba0d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content/drive/MyDrive/CS5284'\n",
      "/scratch/users/nus/e1329380/cs5284/QA_graph/training-graph-models\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/MyDrive/CS5284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../data_preparation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "\n",
    "# adjust this import accordingly to how you call the script\n",
    "from functions_modified import *\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# visualise learnt intermediate embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "from openTSNE import TSNE\n",
    "# from sklearn.manifold import TSNE # very slow\n",
    "tsne = TSNE(\n",
    "    perplexity=30,\n",
    "    metric=\"euclidean\",\n",
    "    n_jobs=8,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# visualise learnt output embeddings\n",
    "def visualize(h, color):\n",
    "    # z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "    z = tsne.fit(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    print(f\"Number of positive is {sum(color)}\")\n",
    "    print(f\"Total number is {len(color)}\")\n",
    "    s = [0.5 if c==0 else 7 for c in color]\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=1, c=color, cmap=pyplot.jet())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new\n",
    "\n",
    "# train code\n",
    "def train(dataloader):\n",
    "    \"\"\"\n",
    "    one epoch\n",
    "    returns average loss for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    # loop batches from dataloader\n",
    "    for d, (batched_subgraphs, question_embeddings, stacked_labels, node_maps, labels, answer_types) in enumerate(dataloader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batched_subgraphs = batched_subgraphs.to(device)\n",
    "        labels = [label.to(device) for label in labels]\n",
    "\n",
    "        # forward pass\n",
    "        out, x_inter = model(batched_subgraphs)\n",
    "\n",
    "        # calculate loss\n",
    "        batch_loss = 0\n",
    "        # batch_valid_nodes = 0\n",
    "        for i, (label, answer_type) in enumerate(zip(labels, answer_types)):\n",
    "            # Create mask for nodes that belong to the ith subgraph\n",
    "            node_mask = (batched_subgraphs.batch == i)\n",
    "            logits = out[node_mask]\n",
    "            target = label\n",
    "\n",
    "            # Create mask that keeps node types same as the answer type\n",
    "            type_mask = torch.tensor([nt == answer_type for nt in batched_subgraphs.node_types[i]], device=device)\n",
    "            masked_logits = logits[type_mask]\n",
    "            masked_labels = target[type_mask].float()\n",
    "\n",
    "            # Skip subgraph if no valid masked nodes\n",
    "            if masked_labels.numel() == 0 or masked_labels.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            pos_weight = torch.tensor([len(masked_labels) / masked_labels.sum()], device=device)\n",
    "            loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "            # normalize by number of nodes predicted on\n",
    "            # This ensures stability, even if subgraphs vary significantly in the number of valid nodes.\n",
    "            batch_loss += loss_fn(masked_logits, masked_labels) / len(masked_labels)\n",
    "            # batch_valid_nodes += len(masked_labels)\n",
    "\n",
    "        # if batch_valid_nodes > 0:\n",
    "        # normalize by number of nodes predicted on\n",
    "        # This ensures stability, even if subgraphs vary significantly in the number of valid nodes.\n",
    "        # batch_loss = batch_loss / batch_valid_nodes\n",
    "        # backward pass and optimization step\n",
    "        batch_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        # shift back to cpu\n",
    "        batch_loss = batch_loss.detach().cpu()\n",
    "        output_cpu = out.detach().cpu()\n",
    "        x_inter_cpu = x_inter.detach().cpu()\n",
    "        labels_cpu = [label.detach().cpu() for label in labels]\n",
    "\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "        # print batch loss every 5 steps\n",
    "        if d%5 == 0:\n",
    "            print('Batch loss is', batch_loss.item())\n",
    "    \n",
    "        # # visualise last batch in the epoch\n",
    "        # if d == len(dataloader) - 1:\n",
    "        #     print(f\"Duration for one epoch is {(time.time() - start)/60} minutes\")\n",
    "        #     visualize(x_inter_cpu, color=torch.cat(labels_cpu, dim=0))\n",
    "    \n",
    "    torch.cuda.empty_cache() # help clear cache taking up cuda space\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# evaluation code\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batched_subgraphs, question_embeddings, stacked_labels, node_maps, labels, answer_types in dataloader:\n",
    "            batched_subgraphs = batched_subgraphs.to(device)\n",
    "            labels = [label for label in labels]\n",
    "\n",
    "            out, _ = model(batched_subgraphs)\n",
    "            output_cpu = out.detach().cpu()\n",
    "            \n",
    "            # calculate accuracy for each subgraph\n",
    "            for i, (label, answer_type) in enumerate(zip(labels, answer_types)):\n",
    "                # Create mask for nodes that belong to the ith subgraph\n",
    "                node_mask = (batched_subgraphs.batch == i).detach().cpu()\n",
    "                # Create mask that keeps node types same as the answer type\n",
    "                type_mask = torch.tensor([nt == answer_type for nt in batched_subgraphs.node_types[i]], device=\"cpu\")\n",
    "                preds = (torch.sigmoid(output_cpu[node_mask][type_mask]) > 0.5).int()\n",
    "                label = label[type_mask]\n",
    "                \n",
    "                all_preds.extend(preds.tolist())\n",
    "                all_labels.extend(label.tolist())\n",
    "                correct += (preds == label).sum().item()\n",
    "                total += label.size(0)\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='binary') # for positive class\n",
    "    recall = recall_score(all_labels, all_preds, average='binary') # for positive class\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary') # for positive class\n",
    "    accuracy = correct / total # biased towards 0\n",
    "\n",
    "    torch.cuda.empty_cache() # help clear cache taking up cuda space\n",
    "\n",
    "    return accuracy, precision, recall, f1, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train code\n",
    "# def train(dataloader):\n",
    "#     \"\"\"\n",
    "#     one epoch\n",
    "#     returns average loss for one epoch\n",
    "#     \"\"\"\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     start = time.time()\n",
    "    \n",
    "#     # loop batches from dataloader\n",
    "#     for d, (batched_subgraphs, labels) in enumerate(dataloader):\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         batched_subgraphs = batched_subgraphs.to(device)\n",
    "#         labels = [label.to(device) for label in labels]\n",
    "\n",
    "#         # forward pass\n",
    "#         out, x_inter = model(batched_subgraphs)\n",
    "\n",
    "#         # calculate loss\n",
    "#         batch_loss = 0\n",
    "#         for i, label in enumerate(labels):\n",
    "#             node_mask = (batched_subgraphs.batch == i)\n",
    "#             logits = out[node_mask]\n",
    "#             target = label.float()\n",
    "\n",
    "#             pos_weight = torch.tensor([len(target) / target.sum()], device=device)\n",
    "#             loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "#             batch_loss += loss_fn(logits, target)\n",
    "\n",
    "#         # backward pass and optimization step\n",
    "#         batch_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # shift back to cpu\n",
    "#         batch_loss = batch_loss.detach().cpu()\n",
    "#         output_cpu = out.detach().cpu()\n",
    "#         x_inter_cpu = x_inter.detach().cpu()\n",
    "#         labels_cpu = [label.detach().cpu() for label in labels]\n",
    "\n",
    "#         total_loss += batch_loss.item()\n",
    "\n",
    "#         # print batch loss every 5 steps\n",
    "#         if d%5 == 0:\n",
    "#             print('Batch loss is', batch_loss.item())\n",
    "        \n",
    "#         # visualise last batch in the epoch\n",
    "#         if d == len(dataloader) - 1:\n",
    "#             print(f\"Duration for one epoch is {(time.time() - start)/60} minutes\")\n",
    "#             visualize(x_inter_cpu, color=torch.cat(labels_cpu, dim=0))\n",
    "    \n",
    "#     torch.cuda.empty_cache() # help clear cache taking up cuda space\n",
    "\n",
    "#     return total_loss / len(dataloader)\n",
    "\n",
    "# # evaluation code\n",
    "# def evaluate(dataloader):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batched_subgraphs, labels in dataloader:\n",
    "#             batched_subgraphs = batched_subgraphs.to(device)\n",
    "#             labels = [label for label in labels]\n",
    "\n",
    "#             out, _ = model(batched_subgraphs)\n",
    "#             output_cpu = out.detach().cpu()\n",
    "\n",
    "#             # calculate accuracy for each subgraph\n",
    "#             for i, label in enumerate(labels):\n",
    "#                 node_mask = (batched_subgraphs.batch == i).detach().cpu()\n",
    "#                 preds = (torch.sigmoid(output_cpu[node_mask]) > 0.5).int()\n",
    "                \n",
    "#                 all_preds.extend(preds.tolist())\n",
    "#                 all_labels.extend(label.tolist())\n",
    "#                 correct += (preds == label).sum().item()\n",
    "#                 total += label.size(0)\n",
    "\n",
    "#     precision = precision_score(all_labels, all_preds, average='binary') # for positive class\n",
    "#     recall = recall_score(all_labels, all_preds, average='binary') # for positive class\n",
    "#     f1 = f1_score(all_labels, all_preds, average='binary') # for positive class\n",
    "#     accuracy = correct / total # biased towards 0\n",
    "\n",
    "#     torch.cuda.empty_cache() # help clear cache taking up cuda space\n",
    "\n",
    "#     return accuracy, precision, recall, f1, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure\n",
    "def focal_loss(logits, labels, alpha=0.25, gamma=2.0):\n",
    "    \"\"\"\n",
    "    NOT USED.\n",
    "    logits: Predicted output from the model (after log-softmax).\n",
    "    labels: Ground truth labels (0 or 1).\n",
    "    alpha: Balancing factor for the minority class.\n",
    "    gamma: Focusing parameter for adjusting the rate at which easy examples are down-weighted.\n",
    "    \"\"\"\n",
    "    # Compute cross-entropy loss per example\n",
    "    ce_loss = F.nll_loss(logits, labels, reduction='none')\n",
    "    # Probabilities for each example\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    # Apply the focal loss adjustment\n",
    "    focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n",
    "\n",
    "    # Return the mean loss\n",
    "    return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new\n",
    "\n",
    "# load data\n",
    "path_to_node_embed = '../Datasets/MetaQA_dataset/processed/node2vec _embeddings/ud_node2vec_embeddings.txt'\n",
    "path_to_idxes = '../Datasets/MetaQA_dataset/processed/idxes.json'\n",
    "path_to_qa = '../Datasets/MetaQA_dataset/vanilla 3-hop/qa_train.txt'\n",
    "path_to_ans_types = '../Datasets/MetaQA_dataset/processed/train_ans_type.txt'\n",
    "\n",
    "# train\n",
    "data = KGQADataset(path_to_node_embed, path_to_idxes, path_to_qa, path_to_ans_types, train = True)\n",
    "dataloader_train = DataLoader(data, batch_size=64, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "# evaluate\n",
    "test = KGQADataset(path_to_node_embed, path_to_idxes, '../Datasets/MetaQA_dataset/vanilla 3-hop/qa_test.txt', '../Datasets/MetaQA_dataset/processed/dev_ans_type.txt', train = False)\n",
    "dataloader_test = DataLoader(test, batch_size=64, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data\n",
    "# path_to_node_embed = '../Datasets/MetaQA_dataset/processed/node2vec _embeddings/ud_node2vec_embeddings.txt'\n",
    "# path_to_idxes = '../Datasets/MetaQA_dataset/processed/idxes.json'\n",
    "# path_to_qa = '../Datasets/MetaQA_dataset/vanilla 3-hop/qa_train.txt'\n",
    "\n",
    "# # train\n",
    "# data = KGQADataset(path_to_node_embed, path_to_idxes, path_to_qa)\n",
    "# sub_data1 = torch.utils.data.Subset(data, list(range(3000)))\n",
    "# dataloader_train = DataLoader(sub_data1, batch_size=64, collate_fn=collate_fn, shuffle=True)\n",
    "# # some from train to evaluate\n",
    "# sub_data2 = torch.utils.data.Subset(data, list(range(5000, 5000+400)))\n",
    "# dataloader_val = DataLoader(sub_data2, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "# # some from test to evaluate\n",
    "# test = KGQADataset(path_to_node_embed, path_to_idxes, '../Datasets/MetaQA_dataset/vanilla 3-hop/qa_test.txt')\n",
    "# sub_data3 = torch.utils.data.Subset(test, list(range(400)))\n",
    "# dataloader_test = DataLoader(sub_data3, batch_size=32, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySijWBPZkQS6"
   },
   "source": [
    "### Test training GNN\n",
    "1. Concatenate the question embedding with each node embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2024)\n",
    "random.seed(2024)\n",
    "np.random.seed(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 883,
     "status": "ok",
     "timestamp": 1729823544018,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "AzAOKp2kiGQK"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, node_dim, question_dim, hidden_dim, output_dim=1):\n",
    "        super(GCN, self).__init__()\n",
    "        # 4 layers, 100 hidden_dim\n",
    "        self.conv1 = GCNConv(node_dim + question_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv4 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, batched_subgraphs):\n",
    "        # concatenate question embeddings with node features for each subgraph along feature dimension\n",
    "        question_emb_expanded = []\n",
    "        for subgraph in batched_subgraphs.to_data_list():\n",
    "            subgraph.x = torch.cat((subgraph.x, subgraph.qn.unsqueeze(0).expand(subgraph.x.size(0), -1)), dim=1)\n",
    "            question_emb_expanded.append(subgraph.x)\n",
    "\n",
    "        batched_subgraphs.x = torch.cat(question_emb_expanded, dim=0)\n",
    "        x, edge_index = batched_subgraphs.x, batched_subgraphs.edge_index\n",
    "\n",
    "        # GCN\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x_inter = self.conv3(x, edge_index)\n",
    "        x = F.relu(x_inter)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        # Output logits directly for BCEWithLogitsLoss\n",
    "        return x.squeeze(-1), x_inter\n",
    "\n",
    "# Binary classification (answer candidate or not)\n",
    "model = GCN(node_dim=64, question_dim=384, hidden_dim=100).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 248873,
     "status": "ok",
     "timestamp": 1729785041431,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "vwWWDog5xt8J",
    "outputId": "ec06b29d-fe3a-4c8f-cabe-3db0d521d2ce"
   },
   "outputs": [],
   "source": [
    "# train 2 epoch\n",
    "for epoch in range(2):\n",
    "    loss = train(dataloader_train)\n",
    "    val_accuracy, val_p, val_r, val_f1, _, _ = evaluate(dataloader_val)\n",
    "    test_accuracy, test_p, test_r, test_f1, _, _ = evaluate(dataloader_test)\n",
    "    print(f'Epoch {epoch}, Train Loss: {loss}')\n",
    "    print(f'Validation Accuracy: {val_accuracy:.8f}, Validation P/R/F1: {val_p:.3f}/{val_r:.3f}/{val_f1:.3f}')\n",
    "    print(f'Test Accuracy: {test_accuracy:.8f}, Test P/R/F1: {test_p:.3f}/{test_r:.3f}/{test_f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test training GAT\n",
    "1. Concatenate the question embedding with each node embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2024)\n",
    "random.seed(2024)\n",
    "np.random.seed(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class QuestionAwareGAT(torch.nn.Module):\n",
    "    def __init__(self, node_dim, question_dim, hidden_dim, output_dim=1):\n",
    "        super(QuestionAwareGAT, self).__init__()\n",
    "        # 4 layers, 100 hidden_dim\n",
    "        self.conv1 = GATConv(node_dim + question_dim, hidden_dim, heads=1, concat=True)\n",
    "        self.conv2 = GATConv(hidden_dim, hidden_dim, heads=1, concat=True)\n",
    "        self.conv3 = GATConv(hidden_dim, hidden_dim, heads=1, concat=True)\n",
    "        self.conv4 = GATConv(hidden_dim, output_dim, heads=1, concat=True)\n",
    "\n",
    "    def forward(self, batched_subgraphs):\n",
    "        # concatenate question embeddings with node features for each subgraph along feature dimension\n",
    "        question_emb_expanded = []\n",
    "        for subgraph in batched_subgraphs.to_data_list():\n",
    "            subgraph.x = torch.cat((subgraph.x, subgraph.qn.unsqueeze(0).expand(subgraph.x.size(0), -1)), dim=1)\n",
    "            question_emb_expanded.append(subgraph.x)\n",
    "\n",
    "        batched_subgraphs.x = torch.cat(question_emb_expanded, dim=0)\n",
    "        x, edge_index = batched_subgraphs.x, batched_subgraphs.edge_index\n",
    "\n",
    "        # GAT\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x_inter = self.conv3(x, edge_index)\n",
    "        x = F.relu(x_inter)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        # Output logits directly for BCEWithLogitsLoss\n",
    "        return x.squeeze(-1), x_inter\n",
    "\n",
    "# Binary classification (answer candidate or not)\n",
    "model = QuestionAwareGAT(node_dim=64, question_dim=384, hidden_dim=100).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss is 1.6537309885025024\n",
      "Batch loss is 1.5430313348770142\n",
      "Batch loss is 1.222909688949585\n",
      "Batch loss is 1.6415810585021973\n",
      "Batch loss is 1.2838495969772339\n",
      "Batch loss is 1.3589661121368408\n",
      "Batch loss is 1.6335830688476562\n",
      "Batch loss is 1.3502583503723145\n",
      "Batch loss is 1.2371432781219482\n",
      "Batch loss is 1.453822135925293\n",
      "Batch loss is 1.321313500404358\n",
      "Batch loss is 1.4179054498672485\n",
      "Batch loss is 1.1516602039337158\n",
      "Batch loss is 1.0206462144851685\n",
      "Batch loss is 1.3867830038070679\n",
      "Batch loss is 1.1479699611663818\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      3\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train(dataloader_train)\n\u001b[0;32m----> 4\u001b[0m     val_accuracy, val_p, val_r, val_f1, _, _ \u001b[38;5;241m=\u001b[39m evaluate(\u001b[43mdataloader_val\u001b[49m)\n\u001b[1;32m      5\u001b[0m     test_accuracy, test_p, test_r, test_f1, _, _ \u001b[38;5;241m=\u001b[39m evaluate(dataloader_test)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader_val' is not defined"
     ]
    }
   ],
   "source": [
    "# train 2 epoch\n",
    "for epoch in range(2):\n",
    "    loss = train(dataloader_train)\n",
    "    val_accuracy, val_p, val_r, val_f1, _, _ = evaluate(dataloader_val)\n",
    "    test_accuracy, test_p, test_r, test_f1, _, _ = evaluate(dataloader_test)\n",
    "    print(f'Epoch {epoch}, Train Loss: {loss}')\n",
    "    print(f'Validation Accuracy: {val_accuracy:.8f}, Validation P/R/F1: {val_p:.3f}/{val_r:.3f}/{val_f1:.3f}')\n",
    "    print(f'Test Accuracy: {test_accuracy:.8f}, Test P/R/F1: {test_p:.3f}/{test_r:.3f}/{test_f1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 1.4612611160625386\n",
      "Test Accuracy: 0.38234432, Test P/R/F1: 0.003/0.596/0.005\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, test_p, test_r, test_f1, _, _ = evaluate(dataloader_test)\n",
    "print(f'Epoch {epoch}, Train Loss: {loss}')\n",
    "print(f'Test Accuracy: {test_accuracy:.8f}, Test P/R/F1: {test_p:.3f}/{test_r:.3f}/{test_f1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyMZY9ko1UI8o+ma2TA1nv4K",
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
