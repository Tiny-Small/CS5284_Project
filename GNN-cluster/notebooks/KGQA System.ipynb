{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4edfd7c9",
   "metadata": {},
   "source": [
    "Load environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2cbf596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/svu/e0315913/.local/lib/python3.8/site-packages')\n",
    "sys.path.append('/home/svu/e0315913/.local/bin')\n",
    "sys.path.append(\"/hpctmp/e0315913/CS5284_Project/GNN-cluster\")\n",
    "\n",
    "import os\n",
    "os.chdir('/hpctmp/e0315913/CS5284_Project/GNN-cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c7ec7",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "59c8c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aa2e4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.config import load_config, validate_config\n",
    "from src.utils.evaluation import evaluate\n",
    "from src.models.alpha import FullOutput, Metrics, threshold_based_candidates, calculate_avg_metrics\n",
    "from src.my_datasets.kgqa_dataset import KGQADataset\n",
    "from src.my_datasets.data_utils import collate_fn\n",
    "from src.models.rgcn_model import RGCNModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ca79a2",
   "metadata": {},
   "source": [
    "Set config and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f6c3b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"config/demo_config.yaml\"\n",
    "\n",
    "config = load_config(CONFIG_PATH)\n",
    "required_keys = [\n",
    "    'model','train', 'node_embed', 'idxes',\n",
    "    'train_qa_data', 'test_qa_data', 'num_hops',\n",
    "    'model_path'\n",
    "]\n",
    "validate_config(config, required_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f4bdf9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2024)\n",
    "random.seed(2024)\n",
    "np.random.seed(2024)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e447d",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c90a6e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KGQADataset(\n",
    "    path_to_node_embed=config['node_embed'],\n",
    "    path_to_idxes=config['idxes'],\n",
    "    path_to_qa=config['train_qa_data'],\n",
    "    path_to_kb=config['raw_kb'],\n",
    "    from_paths_activate=config['from_paths_activate'],\n",
    "    entity_sbert=config['entity_sbert'],\n",
    "    k=config['num_hops']\n",
    ")\n",
    "num_relations = train_dataset.num_relations # extract the num_relation from the entire graph\n",
    "sub_train_dataset = Subset(train_dataset, list(range(config['train']['start_idx'],\n",
    "                                                     config['train']['end_idx'])))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    sub_train_dataset,\n",
    "    batch_size=config['train']['batch_size'],\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b779c9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['train']['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "60603676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x2b92a1f63cd0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0ef38c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_361799/2014548897.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(config['model_path'])\n"
     ]
    }
   ],
   "source": [
    "model = RGCNModel(\n",
    "            node_dim=config['model']['in_channels'],\n",
    "            question_dim=train_dataset.q_embeddings.size(-1),\n",
    "            hidden_dim=config['model']['hidden_channels'],\n",
    "            num_relations=num_relations,\n",
    "            output_dim=config['model']['out_channels'],\n",
    "            num_rgcn=config['model']['num_layers'],\n",
    "            reduced_qn_dim=config['model']['reduced_qn_dim'],\n",
    "            reduced_node_dim=config['model']['reduced_node_dim'],\n",
    "            output_embedding=config['model']['output_embedding'],\n",
    "            use_residuals=config['model']['use_residuals']\n",
    "        )\n",
    "\n",
    "checkpoint = torch.load(config['model_path'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cb1d1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_subgraph_weighting = config['train']['equal_subgraph_weighting']\n",
    "threshold_value = config['threshold_value']\n",
    "hits_at_k = config['train']['hits_at_k']\n",
    "\n",
    "save_all_path = config['save_all_path']\n",
    "save_emb_path = config['save_emb_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d174a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subgraph_qemb(dataloader, model, device, equal_subgraph_weighting, threshold_value, save_all_path, save_emb_path):\n",
    "    model.eval()\n",
    "    \n",
    "    all_batched_subgraphs = []\n",
    "    all_question_embeddings = []\n",
    "    all_candidates_masks = []\n",
    "    all_similarity_scores = []\n",
    "    all_node_maps = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batched_subgraphs, question_embeddings, stacked_labels, node_maps, labels in tqdm(dataloader, desc=\"Extracting subgraph\", leave=True):\n",
    "            # Move tensors to the specified device\n",
    "            batched_subgraphs = batched_subgraphs.to(device)\n",
    "            question_embeddings = question_embeddings.to(device)\n",
    "            stacked_labels = stacked_labels.to(device)\n",
    "\n",
    "            # Perform forward pass\n",
    "            full_output = model(batched_subgraphs, question_embeddings)\n",
    "            output = full_output.output if hasattr(full_output, 'output') else full_output\n",
    "            threshold = full_output.threshold if hasattr(full_output, 'threshold') else threshold_value\n",
    "\n",
    "            # Determine candidate nodes based on similarity threshold\n",
    "            candidates_mask, similarity_scores = threshold_based_candidates(output, threshold=threshold)\n",
    "\n",
    "            # Save batched data to lists (detaching to avoid memory leaks)\n",
    "            all_batched_subgraphs.append(batched_subgraphs.x.detach().cpu())\n",
    "            all_question_embeddings.append(question_embeddings.detach().cpu())\n",
    "            all_candidates_masks.append(candidates_mask.detach().cpu())\n",
    "            all_node_maps.extend(node_maps)  # Collect node maps\n",
    "            all_labels.extend(labels)  # Collect labels\n",
    "            if similarity_scores is not None:\n",
    "                all_similarity_scores.append(similarity_scores.detach().cpu())\n",
    "                \n",
    "    # Concatenate all batched data along the 0-axis (vertically)\n",
    "    all_batched_subgraphs = torch.cat(all_batched_subgraphs, dim=0)\n",
    "    all_question_embeddings = torch.cat(all_question_embeddings, dim=0)\n",
    "    all_candidates_masks = torch.cat(all_candidates_masks, dim=0)\n",
    "    all_similarity_scores = torch.cat(all_similarity_scores, dim=0) if all_similarity_scores else None\n",
    "\n",
    "    # Saving processed data to files\n",
    "    save_subg_qemb_file(all_batched_subgraphs, all_question_embeddings, file_path=save_emb_path)\n",
    "    save_all_to_file(all_batched_subgraphs, all_question_embeddings, all_candidates_masks, all_similarity_scores, all_node_maps, all_labels, file_path=save_all_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_to_file(batched_subgraphs, question_embeddings, candidates_mask, similarity_scores, node_map, labels, file_path):\n",
    "    \n",
    "    data = {\n",
    "        \"batched_subgraphs\": batched_subgraphs,\n",
    "        \"question_embeddings\" : question_embeddings,\n",
    "        \"candidates_mask\": candidates_mask.tolist(),\n",
    "        \"similarity_scores\": similarity_scores.tolist() if similarity_scores is not None else None,\n",
    "        \"node_maps\" : node_map,\n",
    "        \"labels\" : labels\n",
    "    }\n",
    "    \n",
    "    torch.save(data, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2c5463f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_subg_qemb_file(batched_subgraphs, question_embeddings, file_path):\n",
    "    \n",
    "    data = {\n",
    "        \"batched_subgraphs\": batched_subgraphs,\n",
    "        \"question_embeddings\" : question_embeddings,\n",
    "    }\n",
    "    \n",
    "    torch.save(data, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b211d1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting subgraph: 100%|██████████| 4/4 [00:00<00:00,  7.74it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_subgraph_qemb(train_loader, model, device, equal_subgraph_weighting, threshold_value, save_all_path, save_emb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e68d6ac",
   "metadata": {},
   "source": [
    "To load the node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8977bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_361799/4212741518.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  subgraph_qemb = torch.load(save_emb_path)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# from GNN-cluster/data/demo/candidate_metadata.pt\n",
    "def load_saved_data(file_path):\n",
    "    # Load the data from the saved file\n",
    "    saved_data = torch.load(file_path)\n",
    "    \n",
    "    # Extract each component from the dictionary\n",
    "    batched_subgraphs = saved_data[\"batched_subgraphs\"]\n",
    "    question_embeddings = saved_data[\"question_embeddings\"]\n",
    "    candidates_masks = saved_data[\"candidates_masks\"]\n",
    "    similarity_scores = saved_data.get(\"similarity_scores\", None)  # Use .get() in case it's None\n",
    "    node_maps = saved_data[\"node_maps\"]\n",
    "    labels = saved_data[\"labels\"]\n",
    "    \n",
    "    return batched_subgraphs, question_embeddings, candidates_masks, similarity_scores, node_maps, labels\n",
    "\n",
    "\n",
    "# from GNN-cluster/data/demo/subgraph_qembedding.pt\n",
    "def load_saved_data(file_path):\n",
    "    # Load the data from the saved file\n",
    "    saved_data = torch.load(file_path)\n",
    "    \n",
    "    # Extract each component from the dictionary\n",
    "    batched_subgraphs = saved_data[\"batched_subgraphs\"]\n",
    "    question_embeddings = saved_data[\"question_embeddings\"]\n",
    "    \n",
    "    return batched_subgraphs, question_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
