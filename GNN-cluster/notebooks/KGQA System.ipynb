{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4edfd7c9",
   "metadata": {},
   "source": [
    "Load environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2cbf596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/svu/e0315913/.local/lib/python3.8/site-packages')\n",
    "sys.path.append('/home/svu/e0315913/.local/bin')\n",
    "sys.path.append(\"/hpctmp/e0315913/CS5284_Project/GNN-cluster\")\n",
    "\n",
    "import os\n",
    "os.chdir('/hpctmp/e0315913/CS5284_Project/GNN-cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c7ec7",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "59c8c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aa2e4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.config import load_config, validate_config\n",
    "from src.utils.evaluation import evaluate\n",
    "from src.models.alpha import FullOutput, Metrics, threshold_based_candidates, calculate_avg_metrics\n",
    "from src.my_datasets.kgqa_dataset import KGQADataset\n",
    "from src.my_datasets.data_utils import collate_fn\n",
    "from src.models.rgcn_model import RGCNModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ca79a2",
   "metadata": {},
   "source": [
    "Set config and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f6c3b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"config/demo_config.yaml\"\n",
    "\n",
    "config = load_config(CONFIG_PATH)\n",
    "required_keys = [\n",
    "    'model','train', 'node_embed', 'idxes',\n",
    "    'train_qa_data', 'test_qa_data', 'num_hops',\n",
    "    'model_path'\n",
    "]\n",
    "validate_config(config, required_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f4bdf9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2024)\n",
    "random.seed(2024)\n",
    "np.random.seed(2024)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e447d",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c90a6e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KGQADataset(\n",
    "    path_to_node_embed=config['node_embed'],\n",
    "    path_to_idxes=config['idxes'],\n",
    "    path_to_qa=config['train_qa_data'],\n",
    "    path_to_kb=config['raw_kb'],\n",
    "    from_paths_activate=config['from_paths_activate'],\n",
    "    entity_sbert=config['entity_sbert'],\n",
    "    k=config['num_hops']\n",
    ")\n",
    "num_relations = train_dataset.num_relations # extract the num_relation from the entire graph\n",
    "sub_train_dataset = Subset(train_dataset, list(range(config['train']['start_idx'],\n",
    "                                                     config['train']['end_idx'])))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    sub_train_dataset,\n",
    "    batch_size=config['train']['batch_size'],\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b779c9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['train']['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "60603676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x2b92a1f63cd0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0ef38c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_361799/2014548897.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(config['model_path'])\n"
     ]
    }
   ],
   "source": [
    "model = RGCNModel(\n",
    "            node_dim=config['model']['in_channels'],\n",
    "            question_dim=train_dataset.q_embeddings.size(-1),\n",
    "            hidden_dim=config['model']['hidden_channels'],\n",
    "            num_relations=num_relations,\n",
    "            output_dim=config['model']['out_channels'],\n",
    "            num_rgcn=config['model']['num_layers'],\n",
    "            reduced_qn_dim=config['model']['reduced_qn_dim'],\n",
    "            reduced_node_dim=config['model']['reduced_node_dim'],\n",
    "            output_embedding=config['model']['output_embedding'],\n",
    "            use_residuals=config['model']['use_residuals']\n",
    "        )\n",
    "\n",
    "checkpoint = torch.load(config['model_path'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cb1d1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_subgraph_weighting = config['train']['equal_subgraph_weighting']\n",
    "threshold_value = config['threshold_value']\n",
    "hits_at_k = config['train']['hits_at_k']\n",
    "\n",
    "save_all_path = config['save_all_path']\n",
    "save_emb_path = config['save_emb_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d174a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subgraph_qemb(dataloader, model, device, equal_subgraph_weighting, threshold_value, save_all_path, save_emb_path):\n",
    "    model.eval()\n",
    "    \n",
    "    all_batched_subgraphs = []\n",
    "    all_question_embeddings = []\n",
    "    all_candidates_masks = []\n",
    "    all_similarity_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batched_subgraphs, question_embeddings, stacked_labels, _, _ in tqdm(dataloader, desc=\"Extracting subgraph\", leave=True):\n",
    "            batched_subgraphs = batched_subgraphs.to(device)\n",
    "            question_embeddings = question_embeddings.to(device)\n",
    "            stacked_labels = stacked_labels.to(device)\n",
    "\n",
    "            full_output = model(batched_subgraphs, question_embeddings)  # Forward pass\n",
    "\n",
    "            output = full_output.output if isinstance(full_output, FullOutput) else full_output\n",
    "            threshold = full_output.threshold if isinstance(full_output, FullOutput) else threshold_value\n",
    "\n",
    "            candidates_mask, similarity_scores = threshold_based_candidates(output, threshold=threshold)\n",
    "\n",
    "            # Saving Logic\n",
    "            all_batched_subgraphs.append(batched_subgraphs.x.cpu())\n",
    "            all_question_embeddings.append(question_embeddings.cpu())\n",
    "            all_candidates_masks.append(candidates_mask.cpu())\n",
    "            if similarity_scores is not None:\n",
    "                all_similarity_scores.append(similarity_scores.cpu())\n",
    "                \n",
    "    all_batched_subgraphs = torch.cat(all_batched_subgraphs, dim=0)\n",
    "    all_question_embeddings = torch.cat(all_question_embeddings, dim=0)\n",
    "    all_candidates_masks = torch.cat(all_candidates_masks, dim=0)\n",
    "    all_similarity_scores = torch.cat(all_similarity_scores, dim=0) if all_similarity_scores else None\n",
    "\n",
    "    save_subg_qemb_file(all_batched_subgraphs, all_question_embeddings, file_path=save_emb_path)\n",
    "    save_all_to_file(all_batched_subgraphs, all_question_embeddings, all_candidates_masks, all_similarity_scores, file_path=save_all_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6154ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_to_file(batched_subgraphs, question_embeddings, candidates_mask, similarity_scores, file_path):\n",
    "    \n",
    "    data = {\n",
    "        \"batched_subgraphs\": batched_subgraphs,\n",
    "        \"question_embeddings\" : question_embeddings,\n",
    "        \"candidates_mask\": candidates_mask.tolist(),\n",
    "        \"similarity_scores\": similarity_scores.tolist() if similarity_scores is not None else None\n",
    "    }\n",
    "    \n",
    "    torch.save(data, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2c5463f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_subg_qemb_file(batched_subgraphs, question_embeddings, file_path):\n",
    "    \n",
    "    data = {\n",
    "        \"batched_subgraphs\": batched_subgraphs,\n",
    "        \"question_embeddings\" : question_embeddings,\n",
    "    }\n",
    "    \n",
    "    torch.save(data, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b211d1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting subgraph: 100%|██████████| 4/4 [00:00<00:00,  7.74it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_subgraph_qemb(train_loader, model, device, equal_subgraph_weighting, threshold_value, save_all_path, save_emb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e68d6ac",
   "metadata": {},
   "source": [
    "Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cc8977bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_361799/4212741518.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  subgraph_qemb = torch.load(save_emb_path)\n"
     ]
    }
   ],
   "source": [
    "subgraph_qemb = torch.load(save_emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9a271515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(subgraph_qemb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "be9b476f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batched_subgraphs: <class 'torch.Tensor'>\n",
      "  - shape: torch.Size([17064, 384])\n",
      "question_embeddings: <class 'torch.Tensor'>\n",
      "  - shape: torch.Size([20, 384])\n"
     ]
    }
   ],
   "source": [
    "for key, value in subgraph_qemb.items():\n",
    "        print(f\"{key}: {type(value)}\")\n",
    "        if isinstance(value, torch.Tensor):  # If value is a tensor, display its shape\n",
    "            print(f\"  - shape: {value.shape}\")\n",
    "        else:\n",
    "            print(f\"  - content: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7428e185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_361799/171997195.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  candidate_metadata = torch.load(save_all_path)\n"
     ]
    }
   ],
   "source": [
    "candidate_metadata = torch.load(save_all_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b4cfc7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(candidate_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ad106fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batched_subgraphs: <class 'torch.Tensor'>\n",
      "  - shape: torch.Size([17064, 384])\n",
      "question_embeddings: <class 'torch.Tensor'>\n",
      "  - shape: torch.Size([20, 384])\n",
      "candidates_mask: <class 'list'>\n",
      "  - content: [0, 1, 0, 1, 1, 1, 1, 1, 0, 1]\n",
      "similarity_scores: <class 'list'>\n",
      "  - content: [-0.05353173613548279, 0.2537361681461334, -0.05438895896077156, 0.1414671093225479, 0.2819375991821289, 0.057376377284526825, 0.30428123474121094, 0.1439422369003296, -5.918554961681366e-06, 0.2828298807144165]\n"
     ]
    }
   ],
   "source": [
    "for key, value in candidate_metadata.items():\n",
    "        print(f\"{key}: {type(value)}\")\n",
    "        if isinstance(value, torch.Tensor):  # If value is a tensor, display its shape\n",
    "            print(f\"  - shape: {value.shape}\")\n",
    "        else:\n",
    "            print(f\"  - content: {value[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed985a69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
