{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4edfd7c9",
   "metadata": {},
   "source": [
    "Load environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2cbf596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/svu/e0315913/.local/lib/python3.8/site-packages')\n",
    "sys.path.append('/home/svu/e0315913/.local/bin')\n",
    "sys.path.append(\"/hpctmp/e0315913/demo/CS5284_Project/GNN-cluster/config/demo_config.yaml\")\n",
    "\n",
    "import os\n",
    "os.chdir('/hpctmp/e0315913/demo/CS5284_Project/GNN-cluster/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c7ec7",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c8c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2e4e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from src.utils.config import load_config, validate_config\n",
    "from src.utils.evaluation import evaluate\n",
    "from src.models.alpha import FullOutput, Metrics, threshold_based_candidates, calculate_avg_metrics\n",
    "from src.my_datasets.kgqa_dataset import KGQADataset\n",
    "from src.my_datasets.data_utils import collate_fn\n",
    "from src.models.rgcn_model import RGCNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fafdffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.RAG.kgqa_extractor import extract_subgraph_qemb, load_all_metadata, load_subgraph_data, save_all_to_file, save_subg_qemb_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab087cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.alpha import Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ca79a2",
   "metadata": {},
   "source": [
    "Set config and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c3b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"config/demo_config.yaml\"\n",
    "\n",
    "config = load_config(CONFIG_PATH)\n",
    "required_keys = [\n",
    "    'model','train', 'node_embed', 'idxes',\n",
    "    'train_qa_data', 'test_qa_data', 'num_hops',\n",
    "]\n",
    "validate_config(config, required_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4bdf9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2024)\n",
    "random.seed(2024)\n",
    "np.random.seed(2024)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d8a4002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['train']['start_idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e447d",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "070f89d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, random, torch\n",
    "import pandas as pd\n",
    "\n",
    "from torch_geometric.utils import k_hop_subgraph, subgraph\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_folder = '../hf_model' ### you might need to modify this\n",
    "model = SentenceTransformer(\"/hpctmp/e0315913/CS5284_Project/GNN-cluster/src/models/models--sentence-transformers--multi-qa-MiniLM-L6-cos-v1/snapshots/2d981ed0b0b8591b038d472b10c38b96016aab2e\")\n",
    "# model = SentenceTransformer(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f222cf4",
   "metadata": {},
   "source": [
    "# KGQADataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "86c74ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_node_embed, path_to_idxes, path_to_qa, path_to_kb, from_paths_activate, entity_sbert, k=3):\n",
    "        \"\"\"\n",
    "        Initialize without precomputed subgraphs. Computes k-hop subgraphs on-the-fly.\n",
    "        \"\"\"\n",
    "        self.from_paths_activate = from_paths_activate\n",
    "        self.entity_sbert = entity_sbert\n",
    "\n",
    "        if self.from_paths_activate:\n",
    "            # Load the main graph data\n",
    "            self.G = self.generate_nx_graph(path_to_kb)\n",
    "\n",
    "        # Load the main graph data\n",
    "        self.loaded_entity_to_idx, self.loaded_edge_index, self.loaded_relations = self.load_data_json(path_to_idxes)\n",
    "        self.data = self.create_data_object(self.loaded_edge_index, self.loaded_relations, self.loaded_entity_to_idx)\n",
    "\n",
    "        # Store the global number of unique relations\n",
    "        self.num_relations = len(set(self.loaded_relations))\n",
    "        self.k = k\n",
    "\n",
    "        # Load node2vec and sbert embeddings\n",
    "        self.entity_sbert_embeddings = self.get_entity_sbert_embeddings(self.loaded_entity_to_idx)\n",
    "        self.node2vec_embeddings = self.load_node2vec_embeddings(path_to_node_embed)\n",
    "\n",
    "        # Load question and answer data\n",
    "        self.df = pd.read_csv(path_to_qa, sep='\\t', header=None, names=['question', 'answer'])\n",
    "        self.df['answer'] = self.df['answer'].apply(lambda x: x.split(\"|\"))\n",
    "\n",
    "        # Load sentence embeddings\n",
    "        self.q_embeddings = model.encode(\n",
    "            [q.replace(\"[\", \"\").replace(\"]\", \"\") for q in self.df['question']],\n",
    "            batch_size=128,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the question and answers from the DataFrame\n",
    "        row = self.df.iloc[idx]\n",
    "        question, answers = row['question'], row['answer']\n",
    "\n",
    "        # Step 1: Extract the entity from the question (entity marked in square brackets)\n",
    "        entity = self.extract_entity_from_question(question)\n",
    "        entity_node = self.loaded_entity_to_idx[entity]\n",
    "\n",
    "        # Step 2: Get the question embedding\n",
    "        question_embedding = self.q_embeddings[idx]\n",
    "\n",
    "        # Step 3: Compute the k-hop subgraph around the entity dynamically\n",
    "        subset_node_indices, sub_edge_index, _, edge_mask = self.get_k_hop_subgraph(entity_node)\n",
    "\n",
    "        # Step 4: Construct the subgraph based on these subset indices\n",
    "        subgraph_data, node_map = self.construct_subgraph(subset_node_indices, sub_edge_index, edge_mask)\n",
    "\n",
    "        # Step 5: Get the labels\n",
    "        labels = self.get_labels(answers, node_map)\n",
    "\n",
    "        # Step 6: Add node2vec embeddings to the subgraph data\n",
    "        embedding_dim = 384 if (self.from_paths_activate or self.entity_sbert) else 64\n",
    "        subgraph_data.x = self.get_node_embeddings(node_map, question_embedding, entity, embedding_dim)\n",
    "\n",
    "        return subgraph_data, question_embedding, labels, node_map\n",
    "\n",
    "    def get_entity_sbert_embeddings(self, loaded_entity_to_idx):\n",
    "        # entity names as keys\n",
    "        entities = list(loaded_entity_to_idx.keys())\n",
    "        encoded_values = model.encode(entities, batch_size=128, convert_to_tensor=False).tolist()\n",
    "        out = {e: encoded_values[i] for i, e in enumerate(entities)}\n",
    "        return out\n",
    "\n",
    "    def get_k_hop_subgraph(self, entity_node):\n",
    "        \"\"\"\n",
    "        Compute the k-hop subgraph dynamically for a given entity node.\n",
    "        \"\"\"\n",
    "        subset, sub_edge_index, mapping, edge_mask = k_hop_subgraph(\n",
    "            node_idx=entity_node,\n",
    "            num_hops=self.k,\n",
    "            edge_index=self.data.edge_index,\n",
    "            relabel_nodes=True\n",
    "        )\n",
    "        return subset, sub_edge_index, mapping, edge_mask\n",
    "\n",
    "    def construct_subgraph(self, subset_node_indices, sub_edge_index, edge_mask):\n",
    "        \"\"\"\n",
    "        Construct a subgraph Data object for the given subset of nodes and edges.\n",
    "        \"\"\"\n",
    "        node_map = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(subset_node_indices)}\n",
    "\n",
    "        # # Create subgraph data object\n",
    "        sub_edge_attr = self.data.edge_attr[edge_mask]\n",
    "        subgraph_data = Data(edge_index=sub_edge_index, edge_attr=sub_edge_attr) # sub_edge_index is get_k_hop_subgraph's sub_edge_index\n",
    "        return subgraph_data, node_map\n",
    "\n",
    "    def load_data_json(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data['entity_to_idx'], data['edge_index'], data['relations']\n",
    "\n",
    "    def load_data_pt(self, filename):\n",
    "        \"\"\"\n",
    "        Load the list of subsets (k-hop subgraph node indices) from the .pt file.\n",
    "        \"\"\"\n",
    "        data = torch.load(filename)\n",
    "        if isinstance(data, list):\n",
    "            return data  # Return the list of subsets\n",
    "        else:\n",
    "            raise ValueError(\"Expected a list of k-hop subgraph node indices.\")\n",
    "\n",
    "    def create_data_object(self, edge_index, relations, entity_to_idx):\n",
    "        unique_relations = list(set(relations))\n",
    "        relation_mapping = {relation: index for index, relation in enumerate(unique_relations)}\n",
    "\n",
    "        edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "        # Make the graph undirected by adding reverse edges\n",
    "        undirected_edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1) # comment out if want directed\n",
    "        edge_attr = torch.tensor([relation_mapping[rel] for rel in relations])\n",
    "        # Since we now have more edges (two for each undirected edge), we concat them\n",
    "        undirected_edge_attr = torch.cat([edge_attr, edge_attr], dim=0) # comment out if want directed\n",
    "\n",
    "        # Data(edge_index=undirected_edge_index, edge_attr=undirected_edge_attr, num_nodes=len(entity_to_idx))\n",
    "        # Data(edge_index=edge_index, edge_attr=edge_attr, num_nodes=len(entity_to_idx))\n",
    "        return Data(edge_index=undirected_edge_index, edge_attr=undirected_edge_attr, num_nodes=len(entity_to_idx))\n",
    "\n",
    "    def load_node2vec_embeddings(self, file_path, embedding_dim=64):\n",
    "        embeddings_dict = {}\n",
    "\n",
    "        with open(file_path, 'r') as f:\n",
    "            # Skip the first row\n",
    "            next(f)\n",
    "\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "\n",
    "                # The entity is everything before the embedding, so we use -embedding_dim\n",
    "                entity = \" \".join(parts[:-embedding_dim])  # Join all words before the embedding dimensions\n",
    "                embedding = list(map(float, parts[-embedding_dim:]))  # Convert last parts to float\n",
    "\n",
    "                # Store in the dictionary\n",
    "                embeddings_dict[entity] = embedding\n",
    "\n",
    "        return embeddings_dict\n",
    "\n",
    "    def extract_entity_from_question(self, question):\n",
    "        \"\"\"\n",
    "        Extract the entity that is enclosed in square brackets from the question.\n",
    "        Example: \"What city is [Paris] the capital of?\" -> \"Paris\"\n",
    "        \"\"\"\n",
    "        # assumes one entity of interest in each questin\n",
    "        start = question.find('[') + 1\n",
    "        end = question.find(']')\n",
    "        if start == 0 or end == -1:\n",
    "            raise ValueError(f\"No entity found in the question: {question}\")\n",
    "        return question[start:end]\n",
    "\n",
    "    # def get_k_hop_subgraph(self, node_idx):\n",
    "    #     \"\"\"\n",
    "    #     Get the k-hop subgraph centered around the given node index.\n",
    "    #     node_idx (int): Index of the node representing the entity.\n",
    "    #     \"\"\"\n",
    "    #     # Extract k-hop subgraph from the full graph\n",
    "    #     node_idx = torch.tensor([node_idx], dtype=torch.long)\n",
    "    #     subset, sub_edge_index, _, _ = k_hop_subgraph(\n",
    "    #         node_idx=node_idx,\n",
    "    #         num_hops=self.k,\n",
    "    #         edge_index=self.data.edge_index,\n",
    "    #         relabel_nodes=True\n",
    "    #     )\n",
    "\n",
    "    #     # Create a subgraph Data object\n",
    "    #     # subgraph = Data(x=self.data.x[subset], edge_index=sub_edge_index)\n",
    "    #     subgraph = Data(edge_index=sub_edge_index)\n",
    "\n",
    "    #     # Create a mapping from original node indices to subgraph indices\n",
    "    #     node_map = {original_idx.item(): new_idx for new_idx, original_idx in enumerate(subset)}\n",
    "\n",
    "    #     return subgraph, node_map\n",
    "\n",
    "    def get_labels(self, answers, node_map):\n",
    "        labels = torch.zeros(len(node_map), dtype=torch.long)\n",
    "        for ans in answers:\n",
    "            if ans in self.loaded_entity_to_idx:\n",
    "                ans_idx = self.loaded_entity_to_idx[ans]\n",
    "                if ans_idx in node_map:\n",
    "                    labels[node_map[ans_idx]] = 1\n",
    "        return labels\n",
    "\n",
    "    # def get_node_embeddings(self, node_map):\n",
    "    #     embeddings = [[0.0] * len(self.node2vec_embeddings[next(iter(self.node2vec_embeddings))])]*len(node_map)\n",
    "    #     idx_to_entity = {v: k for k, v in self.loaded_entity_to_idx.items()}\n",
    "\n",
    "    #     for ori, new in node_map.items():\n",
    "    #         if idx_to_entity[ori] in self.node2vec_embeddings:\n",
    "    #             embeddings[new] = self.node2vec_embeddings[idx_to_entity[ori]]\n",
    "    #     return torch.tensor(embeddings, dtype=torch.float)\n",
    "\n",
    "    def get_node_embeddings(self, node_map, question_embedding, entity, embedding_dim, random_init=False):\n",
    "        \"\"\"\n",
    "        Get node embeddings from node2vec. If node not found or random_init is True, create random embeddings.\n",
    "\n",
    "        Args:\n",
    "            node_map: Mapping from original graph node indices to subgraph indices.\n",
    "            question_embedding: Embedding for the query entity.\n",
    "            entity: Query entity index.\n",
    "            embedding_dim: Dimensionality of the embeddings.\n",
    "            random_init: Whether to use random initialization for missing embeddings.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Node embeddings for the subgraph.\n",
    "        \"\"\"\n",
    "        # Initialize all embeddings with independent zero vectors (not references)\n",
    "        embeddings = [np.zeros(embedding_dim).tolist() for _ in range(len(node_map))]\n",
    "        idx_to_entity = {v: k for k, v in self.loaded_entity_to_idx.items()}  # Reverse map\n",
    "\n",
    "        if self.from_paths_activate:\n",
    "            # Get embeddings from paths\n",
    "            result = self.find_best_embedding(self.G, entity, question_embedding)\n",
    "            result[entity] = question_embedding  # Assign question embedding to query entity\n",
    "\n",
    "        for ori, new in node_map.items():\n",
    "            if random_init:\n",
    "                # Randomly initialize embeddings if required\n",
    "                embeddings[new] = [random.uniform(-0.1, 0.1) for _ in range(embedding_dim)]\n",
    "            elif ori in idx_to_entity and idx_to_entity[ori] in self.node2vec_embeddings:\n",
    "                # Ensure the node index maps correctly and exists in node2vec\n",
    "                if self.entity_sbert:\n",
    "                    # Use entity SBERT embeddings\n",
    "                    embeddings[new] = self.entity_sbert_embeddings[idx_to_entity[ori]].tolist()\n",
    "                elif self.from_paths_activate:\n",
    "                    # Use path-based SBERT embeddings\n",
    "                    embeddings[new] = result[idx_to_entity[ori]].tolist()\n",
    "                else:\n",
    "                    # Use node2vec embeddings\n",
    "                    embeddings[new] = self.node2vec_embeddings[idx_to_entity[ori]]\n",
    "\n",
    "        # Convert the list of embeddings to a PyTorch tensor\n",
    "        embeddings = np.array(embeddings)  # Convert to numpy array for consistency\n",
    "        if embeddings.ndim != 2:\n",
    "            raise ValueError(f\"Embeddings array must be 2D but got {embeddings.ndim}D\")\n",
    "\n",
    "        return torch.tensor(embeddings, dtype=torch.float)\n",
    "\n",
    "    def generate_nx_graph(self, path):\n",
    "        \"\"\"\n",
    "        Constructs a networkx directed graph that includes both the original and reverse relations.\n",
    "        Each edge includes the concatenated relations between the same pairs of entities.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(path, sep='|', header=None, names=['entity1', 'relation', 'entity2'])\n",
    "\n",
    "        # Remove duplicates\n",
    "        df_unique = df.drop_duplicates() # 133582 edges after dedup\n",
    "\n",
    "        # Define reverse relations and construct reverse edges\n",
    "        reverse_relations = {\n",
    "        'directed_by': 'directed',\n",
    "        'written_by': 'written',\n",
    "        'starred_actors': 'starring',\n",
    "        'has_tags': 'is_tagged_to',\n",
    "        'has_genre': 'is_genre_of',\n",
    "        'has_imdb_rating': 'is_imdb_rating_of',\n",
    "        'has_imdb_votes': 'is_imdb_votes_of',\n",
    "        'in_language': 'language_of',\n",
    "        'release_year': 'is_released_year_of'\n",
    "        }\n",
    "\n",
    "        reverse_rows = []\n",
    "        for index, row in df_unique.iterrows():\n",
    "            reverse_relation = reverse_relations[row['relation']]\n",
    "            reverse_row = {'entity1': row['entity2'], 'relation': reverse_relation, 'entity2': row['entity1']}\n",
    "            reverse_rows.append(reverse_row)\n",
    "\n",
    "        df_reverse = pd.DataFrame(reverse_rows) # 133582 edges\n",
    "        df_combined = pd.concat([df_unique, df_reverse], ignore_index=True) # 267164 edges\n",
    "\n",
    "        # This step consolidates multiple edges between the same pair of entities into a single edge.\n",
    "        # It concatenates all relation values associated with each pair of entities.\n",
    "        df_final = df_combined.groupby(['entity1', 'entity2'], as_index=False).agg({\n",
    "            'relation': ' and '.join\n",
    "        }) # 249349 edges\n",
    "\n",
    "        # Replace underscores in relation names\n",
    "        df_final['relation'] = df_final['relation'].str.replace('_', ' ')\n",
    "\n",
    "        G = nx.from_pandas_edgelist(df_final, source='entity1', target='entity2', edge_attr='relation', create_using=nx.DiGraph())\n",
    "        # Number of entities: 43234\n",
    "        # Number of edges: 249349\n",
    "        # Number of distinct relations: 38\n",
    "        # Distinct relations: {'release year', 'directed by and written by and starred actors', 'directed by and starred actors', 'has imdb rating', 'is genre of', 'has tags and is tagged to', 'directed by and written by', 'starred actors and starring', 'directed', 'directed and written', 'has imdb votes', 'written by and directed by', 'written and directed', 'in language and language of', 'language of', 'has genre', 'is tagged to', 'has imdb rating and has tags', 'directed and written and starring', 'starred actors', 'starring', 'has tags', 'directed and starring', 'written by and directed by and starred actors', 'written by and written', 'written by', 'in language', 'release year and has tags', 'written and directed and starring', 'written and starring', 'is released year of and is tagged to', 'directed by', 'is imdb rating of', 'is imdb rating of and is tagged to', 'written', 'is released year of', 'written by and starred actors', 'is imdb votes of'}\n",
    "\n",
    "        return G\n",
    "\n",
    "    def find_paths(self, G, u, n):\n",
    "        \"\"\"\n",
    "        Finds paths in a graph G starting from node u with until reaching a maximum length of n edges.\n",
    "\n",
    "        Parameters:\n",
    "        G (Graph): The nx graph where entities and relations are defined.\n",
    "        u (str): The starting node for the paths.\n",
    "        n (int): The maximum depth or length of paths in terms of edges.\n",
    "\n",
    "        Returns:\n",
    "        List of paths, where each path is a list of tuples (node, relation) representing\n",
    "        the nodes and relations along the path.\n",
    "        \"\"\"\n",
    "\n",
    "        if n == 0:\n",
    "            return [[(u, None)]]\n",
    "\n",
    "        paths = [\n",
    "            [(u, G[u][neighbor]['relation'])] + path\n",
    "            for neighbor in G.neighbors(u)\n",
    "            for path in self.find_paths(G, neighbor, n - 1)\n",
    "            if u not in [node for node, _ in path] # Avoid cycles\n",
    "        ]\n",
    "        return paths\n",
    "\n",
    "    def find_best_embedding(self, G, query_entity, q_embedding):\n",
    "        \"\"\"\n",
    "        Finds the best path embedding for each unique candidate based on cosine similarity.\n",
    "\n",
    "        Parameters:\n",
    "        G (Graph): The nx graph where entities and relations are defined.\n",
    "        query_entity (str): The entity for which paths are being found.\n",
    "        q_embedding (torch.Tensor): The embedding of the query entity.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary where keys are candidates and values are the best path embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "        paths = self.find_paths(G, query_entity, 2) + self.find_paths(G, query_entity, 1)\n",
    "\n",
    "        sentences = []\n",
    "        candidates = []\n",
    "\n",
    "        for tuple_list in paths:\n",
    "            # Extract the last entity (candidate) in the path\n",
    "            candidate_entity = tuple_list[-1][0]\n",
    "\n",
    "            if candidate_entity != query_entity: # Avoid looping back to the query_entity\n",
    "                candidates.append(candidate_entity)\n",
    "                # Create the sentence for the path\n",
    "                sentence = ' '.join(f\"{tup[0]} {tup[1]}\" if tup[1] else tup[0] for tup in tuple_list)\n",
    "                sentences.append(sentence)\n",
    "\n",
    "        # Calculate path embeddings\n",
    "        path_embeddings = model.encode(sentences, batch_size=128, convert_to_tensor=True)\n",
    "        # Calculate cosine similarities\n",
    "        cosine_scores = util.cos_sim(q_embedding, path_embeddings)[0]\n",
    "\n",
    "        # Dictionary to store the best path embedding for each candidate\n",
    "        best_embeddings = {}\n",
    "        # Dictionary to store the highest cosine score for each candidate\n",
    "        best_scores = {}\n",
    "\n",
    "        for idx, candidate in enumerate(candidates):\n",
    "            cosine_score = cosine_scores[idx].item()\n",
    "\n",
    "            if candidate not in best_embeddings or cosine_score > best_scores[candidate]:\n",
    "                best_scores[candidate] = cosine_score\n",
    "                best_embeddings[candidate] = path_embeddings[idx]\n",
    "\n",
    "        return best_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f998f536",
   "metadata": {},
   "source": [
    "# Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1cfc1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subgraph_qemb(\n",
    "    dataloader, model, device, threshold_value, save_all_path, save_emb_path\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract subgraphs, compute embeddings, and save processed results.\n",
    "\n",
    "    Args:\n",
    "        dataloader: DataLoader for the dataset.\n",
    "        model: The model used to compute embeddings and similarity scores.\n",
    "        device: Device (CPU or GPU) for computations.\n",
    "        threshold_value: Default threshold for filtering candidates.\n",
    "        save_all_path: Path to save all processed data.\n",
    "        save_emb_path: Path to save subgraph and question embeddings.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize lists to store processed data\n",
    "    all_batched_subgraphs = []\n",
    "    all_question_embeddings = []\n",
    "    all_candidates_masks = []\n",
    "    all_similarity_scores = []\n",
    "    all_node_maps = []\n",
    "    all_labels = []\n",
    "    all_output_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (\n",
    "            batched_subgraphs,\n",
    "            question_embeddings,\n",
    "            stacked_labels,\n",
    "            node_maps,\n",
    "            labels,\n",
    "        ) in tqdm(dataloader, desc=\"Extracting subgraph\", leave=True):\n",
    "            # Perform forward pass\n",
    "            batched_subgraphs = batched_subgraphs.to(device)\n",
    "            question_embeddings = question_embeddings.to(device)\n",
    "            stacked_labels = stacked_labels.to(device)\n",
    "            full_output = model(batched_subgraphs, question_embeddings)\n",
    "\n",
    "            # Extract relevant outputs\n",
    "            output = full_output.output if hasattr(full_output, \"output\") else full_output\n",
    "            threshold = full_output.threshold if hasattr(full_output, \"threshold\") else threshold_value\n",
    "            candidates_mask, similarity_score = threshold_based_candidates(output, threshold=threshold)\n",
    "            output_embedding = (\n",
    "                full_output.node_embedding if hasattr(full_output, \"node_embedding\") else output\n",
    "            )\n",
    "\n",
    "            # Store results as lists (detached from PyTorch tensors)\n",
    "            all_batched_subgraphs.append(batched_subgraphs.x.tolist())\n",
    "            all_question_embeddings.append(question_embeddings.tolist())\n",
    "            all_candidates_masks.append(candidates_mask.tolist())\n",
    "            all_node_maps.extend(node_maps)\n",
    "            all_labels.extend(labels)\n",
    "            all_output_embeddings.append(output_embedding.tolist())\n",
    "\n",
    "            # Append similarity scores if available\n",
    "            if similarity_score is not None:\n",
    "                all_similarity_scores.append(similarity_score.tolist())\n",
    "            else:\n",
    "                print(\"Skipping batch with no similarity scores.\")\n",
    "\n",
    "    # Save embeddings and processed data\n",
    "    save_subg_qemb_file(\n",
    "        all_batched_subgraphs,\n",
    "        all_question_embeddings,\n",
    "        file_path=save_emb_path,\n",
    "    )\n",
    "    save_all_to_file(\n",
    "        all_batched_subgraphs,\n",
    "        all_question_embeddings,\n",
    "        all_candidates_masks,\n",
    "        all_similarity_scores,\n",
    "        all_node_maps,\n",
    "        all_labels,\n",
    "        all_output_embeddings,\n",
    "        file_path=save_all_path,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "583b7bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_to_file(\n",
    "    batched_subgraphs,\n",
    "    question_embeddings,\n",
    "    candidates_mask,\n",
    "    similarity_scores,\n",
    "    node_map,\n",
    "    labels,\n",
    "    all_output_embeddings,\n",
    "    file_path,\n",
    "):\n",
    "    data = {\n",
    "        \"batched_subgraphs\": batched_subgraphs,\n",
    "        \"question_embeddings\": question_embeddings,\n",
    "        \"candidates_masks\": candidates_mask,\n",
    "        \"similarity_scores\": similarity_scores,  # Leave as tensor if not None\n",
    "        \"node_maps\": node_map,\n",
    "        \"labels\": labels,\n",
    "        \"all_output_embeddings\": all_output_embeddings,\n",
    "    }\n",
    "\n",
    "    torch.save(data, file_path)\n",
    "\n",
    "\n",
    "def save_subg_qemb_file(\n",
    "    batched_subgraphs, question_embeddings, file_path\n",
    "):\n",
    "\n",
    "    data = {\n",
    "        \"batched_subgraphs\": batched_subgraphs,\n",
    "        \"question_embeddings\": question_embeddings,\n",
    "    }\n",
    "\n",
    "    torch.save(data, file_path)\n",
    "\n",
    "\n",
    "def load_all_metadata(file_path):\n",
    "    # Load the data from the saved file\n",
    "    saved_data = torch.load(file_path)\n",
    "\n",
    "    # Extract each component from the dictionary\n",
    "    batched_subgraphs = saved_data[\"batched_subgraphs\"]\n",
    "    question_embeddings = saved_data[\"question_embeddings\"]\n",
    "    candidates_masks = saved_data[\"candidates_masks\"]\n",
    "    similarity_scores = saved_data.get(\"similarity_scores\", None)\n",
    "    node_maps = saved_data[\"node_maps\"]\n",
    "    labels = saved_data[\"labels\"]\n",
    "    all_output_embeddings = saved_data[\"all_output_embeddings\"]\n",
    "\n",
    "    return (\n",
    "        batched_subgraphs,\n",
    "        question_embeddings,\n",
    "        candidates_masks,\n",
    "        similarity_scores,\n",
    "        node_maps,\n",
    "        labels,\n",
    "        all_output_embeddings,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_subgraph_data(file_path):\n",
    "    # Load the data from the saved file\n",
    "    saved_data = torch.load(file_path)\n",
    "\n",
    "    # Extract each component from the dictionary\n",
    "    batched_subgraphs = saved_data[\"batched_subgraphs\"]\n",
    "    question_embeddings = saved_data[\"question_embeddings\"]\n",
    "\n",
    "    return batched_subgraphs, question_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fdf4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f3d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd8c924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e82a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8855c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a9050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57aef7d8",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c90a6e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KGQADataset(\n",
    "    path_to_node_embed=config['node_embed'],\n",
    "    path_to_idxes=config['idxes'],\n",
    "    path_to_qa=config['train_qa_data'],\n",
    "    path_to_kb=config['raw_kb'],\n",
    "    from_paths_activate=True,\n",
    "    entity_sbert=False,\n",
    "    k=config['num_hops']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8fed52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_relations = train_dataset.num_relations # extract the num_relation from the entire graph\n",
    "sub_train_dataset = Subset(train_dataset, list(range(config['train']['start_idx'],config['train']['end_idx'])))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    sub_train_dataset,\n",
    "    batch_size=config['train']['batch_size'],\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a08c31b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86605/11342916.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(config['model_path_test2'])\n"
     ]
    }
   ],
   "source": [
    "model_test2 = RGCNModel(\n",
    "            node_dim=config['model']['in_channels'],\n",
    "            question_dim=train_dataset.q_embeddings.size(-1),\n",
    "            hidden_dim=config['model']['hidden_channels'],\n",
    "            num_relations=38,\n",
    "            output_dim=config['model']['out_channels'],\n",
    "            num_rgcn=config['model']['num_layers'],\n",
    "            reduced_qn_dim=config['model']['reduced_qn_dim'],\n",
    "            reduced_node_dim=config['model']['reduced_node_dim'],\n",
    "            output_embedding=config['model']['output_embedding'],\n",
    "            use_residuals=config['model']['use_residuals']\n",
    "        )\n",
    "\n",
    "checkpoint = torch.load(config['model_path_test2'])\n",
    "model_test2.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_test2 = model_test2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2de36a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_subgraph_weighting = config['train']['equal_subgraph_weighting']\n",
    "threshold_value = config['threshold_value']\n",
    "hits_at_k = config['train']['hits_at_k']\n",
    "\n",
    "save_all_path = config['save_all_path_test2']\n",
    "save_emb_path = config['save_emb_path_test2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "249420e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting subgraph: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_subgraph_qemb(train_loader, model_test2, device, threshold_value, save_all_path, save_emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5a9484d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RGCNModel(\n",
       "  (fc_reduce_qn): Linear(in_features=384, out_features=64, bias=True)\n",
       "  (reduce_node_dim_layer): RGCNConv(384, 64, num_relations=38)\n",
       "  (input_layer): RGCNConv(128, 64, num_relations=38)\n",
       "  (rgcn_layers): ModuleList(\n",
       "    (0): RGCNConv(64, 64, num_relations=38)\n",
       "  )\n",
       "  (output_layer): RGCNConv(64, 384, num_relations=38)\n",
       ")"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3b81b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batched_subgraphs = []\n",
    "all_question_embeddings = []\n",
    "all_candidates_masks = []\n",
    "all_similarity_scores = []\n",
    "all_node_maps = []\n",
    "all_labels = []\n",
    "all_output_embeddings = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3e1380b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting subgraph: 100%|██████████| 1/1 [00:00<00:00, 47.57it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for (\n",
    "        batched_subgraphs,\n",
    "        question_embeddings,\n",
    "        stacked_labels,\n",
    "        node_maps,\n",
    "        labels,\n",
    "    ) in tqdm(train_loader, desc=\"Extracting subgraph\", leave=True):\n",
    "        batched_subgraphs = batched_subgraphs.to(device)\n",
    "        question_embeddings = question_embeddings.to(device)\n",
    "        stacked_labels = stacked_labels.to(device)\n",
    "        node_maps = node_maps\n",
    "        labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6f0d3dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_output = model_test2(batched_subgraphs, question_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "541c8de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output(node_embedding=tensor([[-0.0455, -0.4252,  0.4711,  ..., -0.1344,  0.3125, -0.2147],\n",
       "        [ 0.2930, -0.4726, -0.2477,  ...,  0.0713,  0.6241, -0.4596],\n",
       "        [ 0.2495, -0.4304, -0.2512,  ..., -0.1379,  0.5478, -0.3119],\n",
       "        ...,\n",
       "        [ 0.2564,  0.4636,  0.8000,  ...,  0.3100, -0.3889,  0.2299],\n",
       "        [ 0.1730, -0.6759,  1.0974,  ..., -0.3653, -0.3239,  0.3315],\n",
       "        [ 0.5554, -0.0330,  0.1109,  ...,  0.1442,  0.1015,  0.1431]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), question_embedding_expanded=tensor([[-0.0327, -0.1177,  0.0200,  ..., -0.0601,  0.0206, -0.0643],\n",
       "        [-0.0327, -0.1177,  0.0200,  ..., -0.0601,  0.0206, -0.0643],\n",
       "        [-0.0327, -0.1177,  0.0200,  ..., -0.0601,  0.0206, -0.0643],\n",
       "        ...,\n",
       "        [-0.0327, -0.1177,  0.0200,  ..., -0.0601,  0.0206, -0.0643],\n",
       "        [-0.0327, -0.1177,  0.0200,  ..., -0.0601,  0.0206, -0.0643],\n",
       "        [-0.0327, -0.1177,  0.0200,  ..., -0.0601,  0.0206, -0.0643]],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3bd74aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = (full_output.output if hasattr(full_output, \"output\") else full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ab820dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output(node_embedding=tensor([[-0.0455, -0.4252,  0.4711,  ..., -0.1344,  0.3125, -0.2147],\n",
       "        [ 0.2930, -0.4726, -0.2477,  ...,  0.0713,  0.6241, -0.4596],\n",
       "        [ 0.2495, -0.4304, -0.2512,  ..., -0.1379,  0.5478, -0.3119],\n",
       "        ...,\n",
       "        [ 0.2564,  0.4636,  0.8000,  ...,  0.3100, -0.3889,  0.2299],\n",
       "        [ 0.1730, -0.6759,  1.0974,  ..., -0.3653, -0.3239,  0.3315],\n",
       "        [ 0.5554, -0.0330,  0.1109,  ...,  0.1442,  0.1015,  0.1431]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), question_embedding_expanded=tensor([[-0.0327, -0.1177,  0.0200,  ..., -0.0601,  0.0206, -0.0643],\n",
       "        [-0.0327, -0.1177,  0.0200,  ..., -0.0601,  0.0206, -0.0643],\n",
       "        [-0.0327, -0.1177,  0.0200,  ..., -0.0601,  0.0206, -0.0643],\n",
       "        ...,\n",
       "        [-0.0327, -0.1177,  0.0200,  ..., -0.0601,  0.0206, -0.0643],\n",
       "        [-0.0327, -0.1177,  0.0200,  ..., -0.0601,  0.0206, -0.0643],\n",
       "        [-0.0327, -0.1177,  0.0200,  ..., -0.0601,  0.0206, -0.0643]],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8accf542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting subgraph:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 242], edge_attr=[242], num_nodes=106, x=[106, 384], batch=[106], ptr=[2])\n",
      "tensor([[-3.2683e-02, -1.1771e-01,  2.0035e-02, -4.9907e-03, -7.1972e-02,\n",
      "          3.1428e-02,  3.0334e-02,  3.4106e-02,  4.0418e-03,  3.7856e-03,\n",
      "         -7.8110e-03,  2.4622e-02,  6.5852e-02, -1.8142e-02,  2.8830e-02,\n",
      "          2.6791e-02,  7.9914e-02,  8.3346e-02,  1.0563e-01,  2.4452e-02,\n",
      "         -2.0814e-02,  2.2093e-02, -1.4475e-02,  2.5309e-02,  5.3784e-03,\n",
      "         -2.8438e-02,  5.9495e-02,  3.7550e-02,  1.2722e-02,  7.6392e-02,\n",
      "         -2.0498e-02,  1.3107e-01, -3.6708e-02, -3.5629e-02,  4.9601e-03,\n",
      "          3.7087e-02, -6.4176e-02, -4.6361e-02, -5.4901e-02, -3.0928e-02,\n",
      "          7.8183e-03,  3.1838e-02,  1.5287e-02,  5.9942e-02,  3.6168e-03,\n",
      "         -6.5001e-02, -4.7575e-02, -2.6547e-02, -6.5822e-03, -5.9673e-02,\n",
      "         -1.4633e-02,  2.7034e-02,  2.7769e-02, -2.5018e-02, -3.8412e-02,\n",
      "          9.1150e-03, -1.2804e-02, -4.1930e-03,  6.5810e-02, -2.5715e-02,\n",
      "         -9.6672e-04, -2.3419e-02,  4.6757e-02, -4.7361e-02,  5.2315e-02,\n",
      "          1.0753e-01, -3.6582e-02, -6.5025e-02,  3.8530e-02, -2.6196e-02,\n",
      "         -1.2152e-02,  6.2801e-02, -4.4540e-02, -7.1486e-02, -3.8994e-02,\n",
      "         -6.8016e-02, -2.2310e-02,  5.3481e-02, -7.6993e-02, -6.6545e-02,\n",
      "          8.9957e-02, -1.0876e-01,  3.2422e-02, -8.1881e-02, -1.5195e-02,\n",
      "          9.1820e-03,  6.1371e-02,  1.0069e-01, -1.0713e-01,  3.4339e-02,\n",
      "          1.6387e-02,  3.1521e-02, -7.4205e-03,  3.6324e-02,  9.8651e-03,\n",
      "         -3.9257e-02, -4.2726e-02, -3.9174e-02,  3.6517e-02,  8.3532e-04,\n",
      "          3.4289e-02, -4.2420e-02, -1.1028e-01, -1.1807e-02,  8.6727e-02,\n",
      "          7.3776e-03,  6.9586e-02, -2.0013e-02, -5.9467e-02, -3.0034e-02,\n",
      "         -8.0359e-03,  4.9114e-02,  1.3529e-02,  1.7339e-02,  3.6847e-02,\n",
      "         -3.9710e-02, -1.9539e-02,  6.2309e-02,  5.2650e-02, -3.2752e-02,\n",
      "         -6.4934e-03, -2.3512e-02, -3.9329e-02, -4.1465e-02, -5.8663e-02,\n",
      "         -3.3782e-02, -4.6538e-02,  1.2995e-31,  4.7512e-02, -4.4775e-02,\n",
      "          4.0338e-02, -4.1470e-02,  3.9250e-02, -5.1933e-02,  1.1372e-03,\n",
      "          7.8396e-03, -1.4532e-02,  7.5990e-03, -6.7691e-02, -8.1935e-02,\n",
      "         -2.5116e-02, -1.6922e-02,  1.1487e-02, -7.1534e-03, -1.1588e-01,\n",
      "          6.2928e-03,  1.5602e-01, -2.4217e-03, -5.3860e-02,  5.8591e-03,\n",
      "         -1.9526e-02,  4.3692e-02, -5.3995e-03,  5.9856e-03,  1.6315e-02,\n",
      "         -3.2714e-03,  4.7390e-02, -5.9053e-02,  3.2885e-03,  8.8112e-02,\n",
      "          1.6011e-02,  1.3544e-01,  7.5685e-03, -8.0489e-02, -6.0815e-02,\n",
      "          6.3940e-02,  3.1356e-02,  7.7823e-02, -5.2638e-02,  7.9941e-03,\n",
      "         -4.7687e-02, -4.3454e-03, -3.6658e-02,  2.8184e-02, -3.0975e-02,\n",
      "          1.4996e-02,  4.6960e-02,  3.5428e-02,  4.4343e-03, -4.5749e-03,\n",
      "          4.1755e-02,  9.9717e-03,  4.3211e-02,  1.3014e-02,  7.5642e-02,\n",
      "         -6.6167e-02,  2.3217e-02,  5.0776e-02, -7.1082e-02,  8.0386e-03,\n",
      "          2.6678e-02, -9.3064e-02, -1.5925e-02,  3.6679e-02,  3.8236e-02,\n",
      "         -2.8218e-02, -7.6137e-02, -3.8714e-02, -5.3836e-02, -5.5165e-02,\n",
      "          1.0481e-02,  1.0682e-02,  6.3635e-02, -1.6496e-02,  2.0350e-03,\n",
      "         -4.1463e-02, -5.9546e-02,  7.9091e-02,  2.5256e-02, -1.2458e-02,\n",
      "          2.2689e-02,  1.7628e-02,  2.8539e-02, -5.1669e-02, -5.4634e-02,\n",
      "         -5.1341e-02,  6.1418e-02,  2.3422e-02,  2.5630e-02,  4.3367e-02,\n",
      "          7.0472e-03, -5.9077e-03,  3.4409e-02, -3.0951e-33,  7.4281e-02,\n",
      "          6.4628e-02, -3.3017e-02,  1.6270e-02,  9.4467e-03, -6.9077e-02,\n",
      "         -1.4261e-02,  5.3250e-03,  1.5610e-01, -1.3182e-02,  2.6630e-02,\n",
      "          2.1585e-02, -4.3002e-02, -5.6214e-02, -3.3330e-02, -1.5847e-02,\n",
      "         -1.9269e-02, -2.4031e-03, -1.2060e-02, -4.3387e-02, -5.8532e-02,\n",
      "          3.5755e-03,  3.0535e-02,  7.6949e-03,  4.9854e-03,  1.1968e-04,\n",
      "          1.5703e-02,  1.0186e-01,  2.0822e-02, -2.2196e-02,  2.6090e-02,\n",
      "         -2.5196e-02, -1.4000e-01, -3.1293e-03, -3.0370e-02, -1.3436e-02,\n",
      "          1.2133e-01, -6.4987e-03,  1.5642e-02,  4.3455e-03, -5.1760e-02,\n",
      "          3.7563e-02,  5.4795e-02,  9.8040e-02,  6.6120e-02, -1.8204e-02,\n",
      "          2.0859e-02,  4.3118e-02, -4.0530e-02, -1.5341e-02, -4.9953e-02,\n",
      "          6.3702e-02,  9.6344e-03, -6.1535e-03, -6.2841e-02, -5.4450e-02,\n",
      "          1.0880e-01, -2.5195e-02,  6.3179e-02,  6.0459e-02,  7.8191e-02,\n",
      "          2.4924e-03,  1.1797e-02, -1.6063e-01, -4.8692e-02,  1.8288e-02,\n",
      "          7.7668e-02,  2.2811e-02,  8.7713e-02,  2.4604e-02, -3.8848e-02,\n",
      "         -6.3630e-02, -2.4252e-02,  1.7944e-02, -4.6434e-02,  7.4390e-02,\n",
      "         -4.5942e-02, -3.9538e-02,  5.1373e-02, -5.7853e-02,  2.1292e-02,\n",
      "          4.7152e-02, -7.5315e-02, -2.4675e-03, -1.9288e-02,  2.1822e-02,\n",
      "          4.3205e-02,  7.0536e-03,  2.7509e-02,  9.4126e-03, -2.7611e-02,\n",
      "          3.0593e-02,  4.2811e-02,  6.1125e-02,  4.1562e-03, -5.1634e-33,\n",
      "         -4.7091e-02,  2.7797e-02, -7.3879e-02, -8.7256e-02,  2.9992e-02,\n",
      "          6.4665e-02,  1.4569e-02,  1.8120e-02,  6.0780e-02,  3.6045e-02,\n",
      "         -4.4999e-02, -1.3504e-02,  3.1470e-03,  5.8570e-02, -4.2318e-02,\n",
      "         -2.7097e-02,  3.1499e-02,  7.2009e-02,  5.7765e-02, -3.1015e-02,\n",
      "         -2.1062e-02, -9.9682e-03,  4.3944e-02, -6.0394e-02,  6.9560e-02,\n",
      "         -9.8514e-02,  2.8438e-02,  9.9796e-03, -4.6918e-02,  5.8917e-02,\n",
      "         -7.2133e-03, -3.2169e-02, -2.5710e-02, -4.3104e-02,  1.2510e-02,\n",
      "         -8.9548e-02,  8.5158e-03,  2.2147e-02, -1.4603e-02, -1.1117e-01,\n",
      "         -5.4874e-02,  1.3327e-01,  2.8324e-02,  3.6439e-02, -2.1492e-02,\n",
      "          5.7295e-04,  1.1867e-01,  2.6845e-02,  6.3417e-02,  3.1461e-03,\n",
      "          3.8277e-02, -5.4434e-02, -1.3733e-01, -7.5544e-02,  3.9768e-02,\n",
      "          1.0384e-01,  1.7388e-02,  5.0767e-02,  7.0114e-03, -1.0542e-01,\n",
      "         -1.3985e-01, -6.0087e-02,  2.0602e-02, -6.4276e-02]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'br' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [164]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(batched_subgraphs)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(question_embeddings)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mbr\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Perform forward pass\u001b[39;00m\n\u001b[1;32m     17\u001b[0m full_output \u001b[38;5;241m=\u001b[39m model(batched_subgraphs, question_embeddings)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'br' is not defined"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for (\n",
    "        batched_subgraphs,\n",
    "        question_embeddings,\n",
    "        stacked_labels,\n",
    "        node_maps,\n",
    "        labels,\n",
    "    ) in tqdm(train_loader, desc=\"Extracting subgraph\", leave=True):\n",
    "            batched_subgraphs = batched_subgraphs.to(device)\n",
    "            question_embeddings = question_embeddings.to(device)\n",
    "            stacked_labels = stacked_labels.to(device)\n",
    "            \n",
    "            print(batched_subgraphs)\n",
    "            print(question_embeddings)\n",
    "            br\n",
    "            # Perform forward pass\n",
    "            full_output = model(batched_subgraphs, question_embeddings)\n",
    "            output = (\n",
    "                full_output.output if hasattr(full_output, \"output\") else full_output\n",
    "            )\n",
    "            threshold = (\n",
    "                full_output.threshold\n",
    "                if hasattr(full_output, \"threshold\")\n",
    "                else threshold_value\n",
    "            )\n",
    "\n",
    "            # Calculate similarity scores and candidates mask\n",
    "            candidates_mask, similarity_score = threshold_based_candidates(\n",
    "                output, threshold=threshold\n",
    "            )\n",
    "            output_embedding = (\n",
    "                full_output.node_embedding\n",
    "                if isinstance(full_output, Output)\n",
    "                else full_output\n",
    "            )\n",
    "            print(candiates_mask, similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "db3c0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = (\n",
    "                full_output.threshold\n",
    "                if hasattr(full_output, \"threshold\")\n",
    "                else threshold_value\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a4df32a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "dfb3086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_mask, similarity_score = threshold_based_candidates(\n",
    "                output, threshold=threshold\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9f5c9ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([106])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d6cc08c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([106])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "012833c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_embedding = (\n",
    "                full_output.node_embedding\n",
    "                if isinstance(full_output, Output)\n",
    "                else full_output\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f0ba04a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batched_subgraphs.append(batched_subgraphs.x.detach().cpu())\n",
    "all_question_embeddings.append(question_embeddings.detach().cpu())\n",
    "all_candidates_masks.append(candidates_mask.detach().cpu())\n",
    "all_node_maps.extend(node_maps)\n",
    "all_labels.extend(labels)\n",
    "all_output_embeddings.append(output_embedding.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6d15b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only append similarity scores if they are not None\n",
    "if similarity_score is not None:\n",
    "    all_similarity_scores.append(\n",
    "        similarity_score.detach().cpu()\n",
    "    )  # Ensures tensor format\n",
    "else:\n",
    "    print(\"Skipping batch with no similarity scores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b018822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batched_subgraphs = torch.cat(all_batched_subgraphs, dim=0)\n",
    "all_question_embeddings = torch.cat(all_question_embeddings, dim=0)\n",
    "all_candidates_masks = torch.cat(all_candidates_masks, dim=0)\n",
    "all_similarity_scores = (\n",
    "    torch.cat(all_similarity_scores, dim=0) if all_similarity_scores else None\n",
    ")\n",
    "\n",
    "original_graph_embeddings = map_subgraph_to_original_graph(\n",
    "    all_batched_subgraphs, all_node_maps\n",
    ")\n",
    "all_output_embeddings = torch.cat(all_output_embeddings, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6caa9a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_candidates_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a7a8f96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0470,  0.2594,  0.2715,  0.2763,  0.1348,  0.5174,  0.4842,  0.4586,\n",
       "         0.4814,  0.4827,  0.4694,  0.4889,  0.4850,  0.1330,  0.1489,  0.2582,\n",
       "         0.3028,  0.3048,  0.2145, -0.0309,  0.2114,  0.1474,  0.2117,  0.2082,\n",
       "         0.4793,  0.4766,  0.4757,  0.4752,  0.1033,  0.1440,  0.1742,  0.1196,\n",
       "         0.2001,  0.3466,  0.3423,  0.3462,  0.3400,  0.5340,  0.5294,  0.5325,\n",
       "         0.4752,  0.5357,  0.5353,  0.4834,  0.5334,  0.0752,  0.1858,  0.0852,\n",
       "         0.1856,  0.1779,  0.1915,  0.4193,  0.4163,  0.4133,  0.4169,  0.4186,\n",
       "         0.4149,  0.4100,  0.4194,  0.4137,  0.4159,  0.0997,  0.2453, -0.0245,\n",
       "         0.2502,  0.2592,  0.2608,  0.2224,  0.4753,  0.5113,  0.4780,  0.4746,\n",
       "         0.4692,  0.4735,  0.4782,  0.4835,  0.4726,  0.1224,  0.2754,  0.2620,\n",
       "         0.2604,  0.2668,  0.2652,  0.3584,  0.3630,  0.3505,  0.3687,  0.3686,\n",
       "         0.3714,  0.3611,  0.3589,  0.3739,  0.3590,  0.3591,  0.3500,  0.3640,\n",
       "         0.3605,  0.3602,  0.2024,  0.1693,  0.1252,  0.1104,  0.1356,  0.1108,\n",
       "         0.0831, -0.0588])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "90bca67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_candidates_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "475a266d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{9445: 0,\n",
       "  21663: 1,\n",
       "  21665: 2,\n",
       "  369: 3,\n",
       "  60: 4,\n",
       "  21666: 5,\n",
       "  21667: 6,\n",
       "  21668: 7,\n",
       "  20389: 8,\n",
       "  21669: 9,\n",
       "  17136: 10,\n",
       "  379: 11,\n",
       "  17131: 12,\n",
       "  55: 13,\n",
       "  180: 14,\n",
       "  27224: 15,\n",
       "  28592: 16,\n",
       "  28865: 17,\n",
       "  30966: 18,\n",
       "  82: 19,\n",
       "  20305: 20,\n",
       "  341: 21,\n",
       "  18597: 22,\n",
       "  17714: 23,\n",
       "  17594: 24,\n",
       "  23124: 25,\n",
       "  16665: 26,\n",
       "  35541: 27,\n",
       "  8085: 28,\n",
       "  42: 29,\n",
       "  39123: 30,\n",
       "  68: 31,\n",
       "  281: 32,\n",
       "  24995: 33,\n",
       "  21968: 34,\n",
       "  16389: 35,\n",
       "  16390: 36,\n",
       "  42128: 37,\n",
       "  22941: 38,\n",
       "  28426: 39,\n",
       "  16463: 40,\n",
       "  16394: 41,\n",
       "  16395: 42,\n",
       "  18976: 43,\n",
       "  20068: 44,\n",
       "  8550: 45,\n",
       "  16772: 46,\n",
       "  34: 47,\n",
       "  18088: 48,\n",
       "  27513: 49,\n",
       "  28291: 50,\n",
       "  16404: 51,\n",
       "  17078: 52,\n",
       "  33214: 53,\n",
       "  16675: 54,\n",
       "  31193: 55,\n",
       "  17305: 56,\n",
       "  19252: 57,\n",
       "  28828: 58,\n",
       "  128: 59,\n",
       "  359: 60,\n",
       "  10: 61,\n",
       "  18587: 62,\n",
       "  2: 63,\n",
       "  21277: 64,\n",
       "  17296: 65,\n",
       "  16660: 66,\n",
       "  137: 67,\n",
       "  30383: 68,\n",
       "  22037: 69,\n",
       "  18591: 70,\n",
       "  20049: 71,\n",
       "  25564: 72,\n",
       "  30384: 73,\n",
       "  21278: 74,\n",
       "  17311: 75,\n",
       "  352: 76,\n",
       "  197: 77,\n",
       "  29022: 78,\n",
       "  194: 79,\n",
       "  29461: 80,\n",
       "  17071: 81,\n",
       "  43036: 82,\n",
       "  22952: 83,\n",
       "  27997: 84,\n",
       "  17853: 85,\n",
       "  17124: 86,\n",
       "  25621: 87,\n",
       "  22244: 88,\n",
       "  23813: 89,\n",
       "  34873: 90,\n",
       "  31637: 91,\n",
       "  43213: 92,\n",
       "  22948: 93,\n",
       "  42315: 94,\n",
       "  43214: 95,\n",
       "  42969: 96,\n",
       "  42970: 97,\n",
       "  9470: 98,\n",
       "  15842: 99,\n",
       "  8226: 100,\n",
       "  12591: 101,\n",
       "  8801: 102,\n",
       "  13700: 103,\n",
       "  10467: 104,\n",
       "  21664: 105}]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_node_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "88ba3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)\n",
      "{9445: 0, 21663: 1, 21665: 2, 369: 3, 60: 4, 21666: 5, 21667: 6, 21668: 7, 20389: 8, 21669: 9, 17136: 10, 379: 11, 17131: 12, 55: 13, 180: 14, 27224: 15, 28592: 16, 28865: 17, 30966: 18, 82: 19, 20305: 20, 341: 21, 18597: 22, 17714: 23, 17594: 24, 23124: 25, 16665: 26, 35541: 27, 8085: 28, 42: 29, 39123: 30, 68: 31, 281: 32, 24995: 33, 21968: 34, 16389: 35, 16390: 36, 42128: 37, 22941: 38, 28426: 39, 16463: 40, 16394: 41, 16395: 42, 18976: 43, 20068: 44, 8550: 45, 16772: 46, 34: 47, 18088: 48, 27513: 49, 28291: 50, 16404: 51, 17078: 52, 33214: 53, 16675: 54, 31193: 55, 17305: 56, 19252: 57, 28828: 58, 128: 59, 359: 60, 10: 61, 18587: 62, 2: 63, 21277: 64, 17296: 65, 16660: 66, 137: 67, 30383: 68, 22037: 69, 18591: 70, 20049: 71, 25564: 72, 30384: 73, 21278: 74, 17311: 75, 352: 76, 197: 77, 29022: 78, 194: 79, 29461: 80, 17071: 81, 43036: 82, 22952: 83, 27997: 84, 17853: 85, 17124: 86, 25621: 87, 22244: 88, 23813: 89, 34873: 90, 31637: 91, 43213: 92, 22948: 93, 42315: 94, 43214: 95, 42969: 96, 42970: 97, 9470: 98, 15842: 99, 8226: 100, 12591: 101, 8801: 102, 13700: 103, 10467: 104, 21664: 105}\n"
     ]
    }
   ],
   "source": [
    "high_similarity_nodes = []\n",
    "for batch_idx, (mask, node_map) in enumerate(zip(all_candidates_masks, all_node_maps)):\n",
    "    print(all_candidates_masks)\n",
    "    print(node_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7acbe637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9445]]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_similarity_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1dd590d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "print(candidate_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfbe512",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.eval()\n",
    "\n",
    "    all_batched_subgraphs = []\n",
    "    all_question_embeddings = []\n",
    "    all_candidates_masks = []\n",
    "    all_similarity_scores = []\n",
    "    all_node_maps = []\n",
    "    all_labels = []\n",
    "    all_output_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (\n",
    "            batched_subgraphs,\n",
    "            question_embeddings,\n",
    "            stacked_labels,\n",
    "            node_maps,\n",
    "            labels,\n",
    "        ) in tqdm(dataloader, desc=\"Extracting subgraph\", leave=True):\n",
    "            # Move tensors to the specified device\n",
    "            batched_subgraphs = batched_subgraphs.to(device)\n",
    "            question_embeddings = question_embeddings.to(device)\n",
    "            stacked_labels = stacked_labels.to(device)\n",
    "\n",
    "            # Perform forward pass\n",
    "            full_output = model(batched_subgraphs, question_embeddings)\n",
    "            output = (\n",
    "                full_output.output if hasattr(full_output, \"output\") else full_output\n",
    "            )\n",
    "            threshold = (\n",
    "                full_output.threshold\n",
    "                if hasattr(full_output, \"threshold\")\n",
    "                else threshold_value\n",
    "            )\n",
    "\n",
    "            # Calculate similarity scores and candidates mask\n",
    "            candidates_mask, similarity_score = threshold_based_candidates(\n",
    "                output, threshold=threshold\n",
    "            )\n",
    "            output_embedding = (\n",
    "                full_output.node_embedding\n",
    "                if isinstance(full_output, Output)\n",
    "                else full_output\n",
    "            )\n",
    "\n",
    "            # Store batched data\n",
    "            all_batched_subgraphs.append(batched_subgraphs.x.detach().cpu())\n",
    "            all_question_embeddings.append(question_embeddings.detach().cpu())\n",
    "            all_candidates_masks.append(candidates_mask.detach().cpu())\n",
    "            all_node_maps.extend(node_maps)\n",
    "            all_labels.extend(labels)\n",
    "            all_output_embeddings.append(output_embedding.detach().cpu())\n",
    "\n",
    "            # Only append similarity scores if they are not None\n",
    "            if similarity_score is not None:\n",
    "                all_similarity_scores.append(\n",
    "                    similarity_score.detach().cpu()\n",
    "                )  # Ensures tensor format\n",
    "            else:\n",
    "                print(\"Skipping batch with no similarity scores.\")\n",
    "\n",
    "    # Concatenate all batched data along the 0-axis (vertically)\n",
    "    all_batched_subgraphs = torch.cat(all_batched_subgraphs, dim=0)\n",
    "    all_question_embeddings = torch.cat(all_question_embeddings, dim=0)\n",
    "    all_candidates_masks = torch.cat(all_candidates_masks, dim=0)\n",
    "    all_similarity_scores = (\n",
    "        torch.cat(all_similarity_scores, dim=0) if all_similarity_scores else None\n",
    "    )\n",
    "\n",
    "    original_graph_embeddings = map_subgraph_to_original_graph(\n",
    "        all_batched_subgraphs, all_node_maps\n",
    "    )\n",
    "    all_output_embeddings = torch.cat(all_output_embeddings, dim=0)\n",
    "    \n",
    "    \n",
    "    high_similarity_nodes = []\n",
    "    for batch_idx, (mask, node_map) in enumerate(zip(all_candidates_masks, all_node_maps)):\n",
    "        # Get the indices of high-similarity candidates in the subgraph\n",
    "        candidate_indices = torch.nonzero(mask, as_tuple=True)[0]  # Indices where mask is True\n",
    "\n",
    "        # Map the subgraph indices to original graph indices using node_map\n",
    "        batch_high_similarity_nodes = [node_map[idx.item()] for idx in candidate_indices]\n",
    "        high_similarity_nodes.append(batch_high_similarity_nodes)\n",
    "    \n",
    "    \n",
    "    save_subg_qemb_file(\n",
    "        all_batched_subgraphs,\n",
    "        original_graph_embeddings,\n",
    "        all_question_embeddings,\n",
    "        file_path=save_emb_path,\n",
    "    )\n",
    "    save_all_to_file(\n",
    "        all_batched_subgraphs,\n",
    "        original_graph_embeddings,\n",
    "        all_question_embeddings,\n",
    "        all_candidates_masks,\n",
    "        all_similarity_scores,\n",
    "        all_node_maps,\n",
    "        all_labels,\n",
    "        all_output_embeddings,\n",
    "        high_similarity_nodes,\n",
    "        file_path=save_all_path,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d85786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83051069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c58754f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88532a28",
   "metadata": {},
   "source": [
    "## Test 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd1291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3cca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51cdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6091dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_to_file(\n",
    "    batched_subgraphs,\n",
    "    original_graph_embeddings,\n",
    "    question_embeddings,\n",
    "    candidates_mask,\n",
    "    similarity_scores,\n",
    "    node_map,\n",
    "    labels,\n",
    "    all_output_embeddings,\n",
    "    file_path,\n",
    "):\n",
    "    data = {\n",
    "        \"batched_subgraphs\": batched_subgraphs,\n",
    "        \"original_graph_embeddings\": original_graph_embeddings,\n",
    "        \"question_embeddings\": question_embeddings,\n",
    "        \"candidates_masks\": candidates_mask,\n",
    "        \"similarity_scores\": similarity_scores,  # Leave as tensor if not None\n",
    "        \"node_maps\": node_map,\n",
    "        \"labels\": labels,\n",
    "        \"all_output_embeddings\" : all_output_embeddings\n",
    "    }\n",
    "\n",
    "    torch.save(data, file_path)\n",
    "\n",
    "\n",
    "def save_subg_qemb_file(\n",
    "    batched_subgraphs, original_graph_embeddings, question_embeddings, file_path\n",
    "):\n",
    "\n",
    "    data = {\n",
    "        \"batched_subgraphs\": batched_subgraphs,\n",
    "        \"original_graph_embeddings\": original_graph_embeddings,\n",
    "        \"question_embeddings\": question_embeddings,\n",
    "    }\n",
    "\n",
    "    torch.save(data, file_path)\n",
    "\n",
    "def extract_subgraph_qemb(dataloader, model, device, threshold_value, save_all_path, save_emb_path):\n",
    "    model.eval()\n",
    "\n",
    "    all_batched_subgraphs = []\n",
    "    all_question_embeddings = []\n",
    "    all_candidates_masks = []\n",
    "    all_similarity_scores = [] \n",
    "    all_node_maps = []\n",
    "    all_labels = []\n",
    "    all_output_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (\n",
    "            batched_subgraphs,\n",
    "            question_embeddings,\n",
    "            stacked_labels,\n",
    "            node_maps,\n",
    "            labels,\n",
    "        ) in tqdm(dataloader, desc=\"Extracting subgraph\", leave=True):\n",
    "            print(node_maps)\n",
    "            # Move tensors to the specified device\n",
    "#             batched_subgraphs = batched_subgraphs.to(device)\n",
    "#             question_embeddings = question_embeddings.to(device)\n",
    "#             stacked_labels = stacked_labels.to(device)\n",
    "\n",
    "#             # Perform forward pass\n",
    "#             full_output = model(batched_subgraphs, question_embeddings)\n",
    "#             output = (\n",
    "#                 full_output.output if hasattr(full_output, \"output\") else full_output\n",
    "#             )\n",
    "#             threshold = (\n",
    "#                 full_output.threshold\n",
    "#                 if hasattr(full_output, \"threshold\")\n",
    "#                 else threshold_value\n",
    "#             )\n",
    "\n",
    "#             # Calculate similarity scores and candidates mask\n",
    "#             candidates_mask, similarity_score = threshold_based_candidates(\n",
    "#                 output, threshold=threshold\n",
    "#             )\n",
    "#             output_embedding = full_output.node_embedding if isinstance(full_output, Output) else full_output\n",
    "\n",
    "\n",
    "#             # Store batched data\n",
    "#             all_batched_subgraphs.append(batched_subgraphs.x.detach().cpu())\n",
    "#             all_question_embeddings.append(question_embeddings.detach().cpu())\n",
    "#             all_candidates_masks.append(candidates_mask.detach().cpu())\n",
    "#             all_node_maps.extend(node_maps)\n",
    "#             all_labels.extend(labels)\n",
    "#             all_output_embeddings.append(output_embedding.detach().cpu())\n",
    "\n",
    "#             # Only append similarity scores if they are not None\n",
    "#             if similarity_score is not None:\n",
    "#                 all_similarity_scores.append(\n",
    "#                     similarity_score.detach().cpu()\n",
    "#                 )  # Ensures tensor format\n",
    "#             else:\n",
    "#                 print(\"Skipping batch with no similarity scores.\")\n",
    "\n",
    "    # Concatenate all batched data along the 0-axis (vertically)\n",
    "    all_batched_subgraphs = torch.cat(all_batched_subgraphs, dim=0)\n",
    "    all_question_embeddings = torch.cat(all_question_embeddings, dim=0)\n",
    "    all_candidates_masks = torch.cat(all_candidates_masks, dim=0)\n",
    "    all_similarity_scores = (\n",
    "        torch.cat(all_similarity_scores, dim=0) if all_similarity_scores else None\n",
    "    )\n",
    "\n",
    "    original_graph_embeddings = map_subgraph_to_original_graph(all_batched_subgraphs, all_node_maps)\n",
    "    all_output_embeddings = torch.cat(all_output_embeddings, dim=0)\n",
    "    print(all_output_embeddings)\n",
    "    save_subg_qemb_file(\n",
    "        all_batched_subgraphs, original_graph_embeddings, all_question_embeddings, file_path=save_emb_path\n",
    "    )\n",
    "    save_all_to_file(\n",
    "        all_batched_subgraphs,\n",
    "        original_graph_embeddings,\n",
    "        all_question_embeddings,\n",
    "        all_candidates_masks,\n",
    "        all_similarity_scores,\n",
    "        all_node_maps,\n",
    "        all_labels,\n",
    "        all_output_embeddings,\n",
    "        file_path=save_all_path,\n",
    "    )\n",
    "\n",
    "\n",
    "def map_subgraph_to_original_graph(all_batched_subgraphs, all_node_maps):\n",
    "    original_graph_embeddings = {}\n",
    "\n",
    "    start_index = 0\n",
    "    for node_map in all_node_maps:\n",
    "        subgraph_size = len(node_map)\n",
    "        subgraph_embeddings = all_batched_subgraphs[start_index : start_index + subgraph_size]\n",
    "        \n",
    "        for original_idx, subgraph_idx in node_map.items():\n",
    "            original_graph_embeddings[original_idx] = subgraph_embeddings[subgraph_idx]\n",
    "\n",
    "        start_index += subgraph_size\n",
    "\n",
    "    return original_graph_embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2622c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_1 = KGQADataset(\n",
    "    path_to_node_embed=config['node_embed'],\n",
    "    path_to_idxes=config['idxes'],\n",
    "    path_to_qa=config['train_qa_data'],\n",
    "    path_to_kb=config['raw_kb'],\n",
    "    from_paths_activate=False,\n",
    "    entity_sbert=True,\n",
    "    k=config['num_hops']\n",
    ")\n",
    "\n",
    "num_relations_1 = train_dataset_1.num_relations # extract the num_relation from the entire graph\n",
    "sub_train_dataset_1 = Subset(train_dataset_1, list(range(config['train']['start_idx'],config['train']['end_idx'])))\n",
    "\n",
    "train_loader_1 = DataLoader(\n",
    "    sub_train_dataset_1,\n",
    "    batch_size=config['train']['batch_size'],\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ce0f782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['train']['start_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "560da2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86605/752439551.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(config['model_path_test8'])\n"
     ]
    }
   ],
   "source": [
    "model_test8 = RGCNModel(\n",
    "            node_dim=config['model']['in_channels'],\n",
    "            question_dim=train_dataset_1.q_embeddings.size(-1),\n",
    "            hidden_dim=config['model']['hidden_channels'],\n",
    "            num_relations=num_relations_1,\n",
    "            output_dim=config['model']['out_channels'],\n",
    "            num_rgcn=config['model']['num_layers'],\n",
    "            reduced_qn_dim=config['model']['reduced_qn_dim'],\n",
    "            reduced_node_dim=config['model']['reduced_node_dim'],\n",
    "            output_embedding=config['model']['output_embedding'],\n",
    "            use_residuals=config['model']['use_residuals']\n",
    "        )\n",
    "\n",
    "checkpoint = torch.load(config['model_path_test8'])\n",
    "model_test8.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_test8 = model_test8.to(device)\n",
    "\n",
    "equal_subgraph_weighting = config['train']['equal_subgraph_weighting']\n",
    "threshold_value = config['threshold_value']\n",
    "hits_at_k = config['train']['hits_at_k']\n",
    "\n",
    "save_all_path_test8 = config['save_all_path_test8']\n",
    "save_emb_path_test8 = config['save_emb_path_test8']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c6d96448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/hpctmp/e0315913/demo/CS5284_Project/GNN-cluster/data/demo/subgraph_qembedding_test8.pt'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_emb_path_test8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "178ddeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting subgraph: 100%|██████████| 1/1 [00:00<00:00, 28.02it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_subgraph_qemb(train_loader_1, \n",
    "                     model_test8,\n",
    "                     device, \n",
    "                     threshold_value,\n",
    "                     save_all_path_test8,\n",
    "                     save_emb_path_test8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dade5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RGCNModel(\n",
       "  (fc_reduce_qn): Linear(in_features=384, out_features=64, bias=True)\n",
       "  (reduce_node_dim_layer): RGCNConv(384, 64, num_relations=9)\n",
       "  (input_layer): RGCNConv(128, 64, num_relations=9)\n",
       "  (rgcn_layers): ModuleList(\n",
       "    (0): RGCNConv(64, 64, num_relations=9)\n",
       "  )\n",
       "  (output_layer): RGCNConv(64, 384, num_relations=9)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test8.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3167e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting subgraph: 100%|██████████| 1/1 [00:00<00:00, 83.59it/s]\n"
     ]
    }
   ],
   "source": [
    "all_batched_subgraphs = []\n",
    "all_question_embeddings = []\n",
    "all_candidates_masks = []\n",
    "all_similarity_scores = []\n",
    "all_node_maps = []\n",
    "all_labels = []\n",
    "all_output_embeddings = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (\n",
    "        batched_subgraphs,\n",
    "        question_embeddings,\n",
    "        stacked_labels,\n",
    "        node_maps,\n",
    "        labels,\n",
    "    ) in tqdm(train_loader_1, desc=\"Extracting subgraph\", leave=True):\n",
    "        batched_subgraphs = batched_subgraphs.to(device)\n",
    "        question_embeddings = question_embeddings.to(device)\n",
    "        stacked_labels = stacked_labels.to(device)\n",
    "        node_maps = node_maps\n",
    "        labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84f3f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_output = model_test8(batched_subgraphs, question_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71748c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = (full_output.output if hasattr(full_output, \"output\") else full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34252983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting subgraph:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 248], edge_attr=[248], x=[106, 384], batch=[106], ptr=[2])\n",
      "tensor([[-3.2683e-02, -1.1771e-01,  2.0035e-02, -4.9907e-03, -7.1972e-02,\n",
      "          3.1428e-02,  3.0334e-02,  3.4106e-02,  4.0418e-03,  3.7856e-03,\n",
      "         -7.8110e-03,  2.4622e-02,  6.5852e-02, -1.8142e-02,  2.8830e-02,\n",
      "          2.6791e-02,  7.9914e-02,  8.3346e-02,  1.0563e-01,  2.4452e-02,\n",
      "         -2.0814e-02,  2.2093e-02, -1.4475e-02,  2.5309e-02,  5.3784e-03,\n",
      "         -2.8438e-02,  5.9495e-02,  3.7550e-02,  1.2722e-02,  7.6392e-02,\n",
      "         -2.0498e-02,  1.3107e-01, -3.6708e-02, -3.5629e-02,  4.9601e-03,\n",
      "          3.7087e-02, -6.4176e-02, -4.6361e-02, -5.4901e-02, -3.0928e-02,\n",
      "          7.8183e-03,  3.1838e-02,  1.5287e-02,  5.9942e-02,  3.6168e-03,\n",
      "         -6.5001e-02, -4.7575e-02, -2.6547e-02, -6.5822e-03, -5.9673e-02,\n",
      "         -1.4633e-02,  2.7034e-02,  2.7769e-02, -2.5018e-02, -3.8412e-02,\n",
      "          9.1150e-03, -1.2804e-02, -4.1930e-03,  6.5810e-02, -2.5715e-02,\n",
      "         -9.6672e-04, -2.3419e-02,  4.6757e-02, -4.7361e-02,  5.2315e-02,\n",
      "          1.0753e-01, -3.6582e-02, -6.5025e-02,  3.8530e-02, -2.6196e-02,\n",
      "         -1.2152e-02,  6.2801e-02, -4.4540e-02, -7.1486e-02, -3.8994e-02,\n",
      "         -6.8016e-02, -2.2310e-02,  5.3481e-02, -7.6993e-02, -6.6545e-02,\n",
      "          8.9957e-02, -1.0876e-01,  3.2422e-02, -8.1881e-02, -1.5195e-02,\n",
      "          9.1820e-03,  6.1371e-02,  1.0069e-01, -1.0713e-01,  3.4339e-02,\n",
      "          1.6387e-02,  3.1521e-02, -7.4205e-03,  3.6324e-02,  9.8651e-03,\n",
      "         -3.9257e-02, -4.2726e-02, -3.9174e-02,  3.6517e-02,  8.3532e-04,\n",
      "          3.4289e-02, -4.2420e-02, -1.1028e-01, -1.1807e-02,  8.6727e-02,\n",
      "          7.3776e-03,  6.9586e-02, -2.0013e-02, -5.9467e-02, -3.0034e-02,\n",
      "         -8.0359e-03,  4.9114e-02,  1.3529e-02,  1.7339e-02,  3.6847e-02,\n",
      "         -3.9710e-02, -1.9539e-02,  6.2309e-02,  5.2650e-02, -3.2752e-02,\n",
      "         -6.4934e-03, -2.3512e-02, -3.9329e-02, -4.1465e-02, -5.8663e-02,\n",
      "         -3.3782e-02, -4.6538e-02,  1.2995e-31,  4.7512e-02, -4.4775e-02,\n",
      "          4.0338e-02, -4.1470e-02,  3.9250e-02, -5.1933e-02,  1.1372e-03,\n",
      "          7.8396e-03, -1.4532e-02,  7.5990e-03, -6.7691e-02, -8.1935e-02,\n",
      "         -2.5116e-02, -1.6922e-02,  1.1487e-02, -7.1534e-03, -1.1588e-01,\n",
      "          6.2928e-03,  1.5602e-01, -2.4217e-03, -5.3860e-02,  5.8591e-03,\n",
      "         -1.9526e-02,  4.3692e-02, -5.3995e-03,  5.9856e-03,  1.6315e-02,\n",
      "         -3.2714e-03,  4.7390e-02, -5.9053e-02,  3.2885e-03,  8.8112e-02,\n",
      "          1.6011e-02,  1.3544e-01,  7.5685e-03, -8.0489e-02, -6.0815e-02,\n",
      "          6.3940e-02,  3.1356e-02,  7.7823e-02, -5.2638e-02,  7.9941e-03,\n",
      "         -4.7687e-02, -4.3454e-03, -3.6658e-02,  2.8184e-02, -3.0975e-02,\n",
      "          1.4996e-02,  4.6960e-02,  3.5428e-02,  4.4343e-03, -4.5749e-03,\n",
      "          4.1755e-02,  9.9717e-03,  4.3211e-02,  1.3014e-02,  7.5642e-02,\n",
      "         -6.6167e-02,  2.3217e-02,  5.0776e-02, -7.1082e-02,  8.0386e-03,\n",
      "          2.6678e-02, -9.3064e-02, -1.5925e-02,  3.6679e-02,  3.8236e-02,\n",
      "         -2.8218e-02, -7.6137e-02, -3.8714e-02, -5.3836e-02, -5.5165e-02,\n",
      "          1.0481e-02,  1.0682e-02,  6.3635e-02, -1.6496e-02,  2.0350e-03,\n",
      "         -4.1463e-02, -5.9546e-02,  7.9091e-02,  2.5256e-02, -1.2458e-02,\n",
      "          2.2689e-02,  1.7628e-02,  2.8539e-02, -5.1669e-02, -5.4634e-02,\n",
      "         -5.1341e-02,  6.1418e-02,  2.3422e-02,  2.5630e-02,  4.3367e-02,\n",
      "          7.0472e-03, -5.9077e-03,  3.4409e-02, -3.0951e-33,  7.4281e-02,\n",
      "          6.4628e-02, -3.3017e-02,  1.6270e-02,  9.4467e-03, -6.9077e-02,\n",
      "         -1.4261e-02,  5.3250e-03,  1.5610e-01, -1.3182e-02,  2.6630e-02,\n",
      "          2.1585e-02, -4.3002e-02, -5.6214e-02, -3.3330e-02, -1.5847e-02,\n",
      "         -1.9269e-02, -2.4031e-03, -1.2060e-02, -4.3387e-02, -5.8532e-02,\n",
      "          3.5755e-03,  3.0535e-02,  7.6949e-03,  4.9854e-03,  1.1968e-04,\n",
      "          1.5703e-02,  1.0186e-01,  2.0822e-02, -2.2196e-02,  2.6090e-02,\n",
      "         -2.5196e-02, -1.4000e-01, -3.1293e-03, -3.0370e-02, -1.3436e-02,\n",
      "          1.2133e-01, -6.4987e-03,  1.5642e-02,  4.3455e-03, -5.1760e-02,\n",
      "          3.7563e-02,  5.4795e-02,  9.8040e-02,  6.6120e-02, -1.8204e-02,\n",
      "          2.0859e-02,  4.3118e-02, -4.0530e-02, -1.5341e-02, -4.9953e-02,\n",
      "          6.3702e-02,  9.6344e-03, -6.1535e-03, -6.2841e-02, -5.4450e-02,\n",
      "          1.0880e-01, -2.5195e-02,  6.3179e-02,  6.0459e-02,  7.8191e-02,\n",
      "          2.4924e-03,  1.1797e-02, -1.6063e-01, -4.8692e-02,  1.8288e-02,\n",
      "          7.7668e-02,  2.2811e-02,  8.7713e-02,  2.4604e-02, -3.8848e-02,\n",
      "         -6.3630e-02, -2.4252e-02,  1.7944e-02, -4.6434e-02,  7.4390e-02,\n",
      "         -4.5942e-02, -3.9538e-02,  5.1373e-02, -5.7853e-02,  2.1292e-02,\n",
      "          4.7152e-02, -7.5315e-02, -2.4675e-03, -1.9288e-02,  2.1822e-02,\n",
      "          4.3205e-02,  7.0536e-03,  2.7509e-02,  9.4126e-03, -2.7611e-02,\n",
      "          3.0593e-02,  4.2811e-02,  6.1125e-02,  4.1562e-03, -5.1634e-33,\n",
      "         -4.7091e-02,  2.7797e-02, -7.3879e-02, -8.7256e-02,  2.9992e-02,\n",
      "          6.4665e-02,  1.4569e-02,  1.8120e-02,  6.0780e-02,  3.6045e-02,\n",
      "         -4.4999e-02, -1.3504e-02,  3.1470e-03,  5.8570e-02, -4.2318e-02,\n",
      "         -2.7097e-02,  3.1499e-02,  7.2009e-02,  5.7765e-02, -3.1015e-02,\n",
      "         -2.1062e-02, -9.9682e-03,  4.3944e-02, -6.0394e-02,  6.9560e-02,\n",
      "         -9.8514e-02,  2.8438e-02,  9.9796e-03, -4.6918e-02,  5.8917e-02,\n",
      "         -7.2133e-03, -3.2169e-02, -2.5710e-02, -4.3104e-02,  1.2510e-02,\n",
      "         -8.9548e-02,  8.5158e-03,  2.2147e-02, -1.4603e-02, -1.1117e-01,\n",
      "         -5.4874e-02,  1.3327e-01,  2.8324e-02,  3.6439e-02, -2.1492e-02,\n",
      "          5.7295e-04,  1.1867e-01,  2.6845e-02,  6.3417e-02,  3.1461e-03,\n",
      "          3.8277e-02, -5.4434e-02, -1.3733e-01, -7.5544e-02,  3.9768e-02,\n",
      "          1.0384e-01,  1.7388e-02,  5.0767e-02,  7.0114e-03, -1.0542e-01,\n",
      "         -1.3985e-01, -6.0087e-02,  2.0602e-02, -6.4276e-02]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'br' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(batched_subgraphs)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(question_embeddings)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mbr\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Perform forward pass\u001b[39;00m\n\u001b[1;32m     17\u001b[0m full_output \u001b[38;5;241m=\u001b[39m model(batched_subgraphs, question_embeddings)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'br' is not defined"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for (\n",
    "        batched_subgraphs,\n",
    "        question_embeddings,\n",
    "        stacked_labels,\n",
    "        node_maps,\n",
    "        labels,\n",
    "    ) in tqdm(train_loader_1, desc=\"Extracting subgraph\", leave=True):\n",
    "            batched_subgraphs = batched_subgraphs.to(device)\n",
    "            question_embeddings = question_embeddings.to(device)\n",
    "            stacked_labels = stacked_labels.to(device)\n",
    "            \n",
    "            print(batched_subgraphs)\n",
    "            print(question_embeddings)\n",
    "            br\n",
    "            # Perform forward pass\n",
    "            full_output = model(batched_subgraphs, question_embeddings)\n",
    "            output = (\n",
    "                full_output.output if hasattr(full_output, \"output\") else full_output\n",
    "            )\n",
    "            threshold = (\n",
    "                full_output.threshold\n",
    "                if hasattr(full_output, \"threshold\")\n",
    "                else threshold_value\n",
    "            )\n",
    "\n",
    "            # Calculate similarity scores and candidates mask\n",
    "            candidates_mask, similarity_score = threshold_based_candidates(\n",
    "                output, threshold=threshold\n",
    "            )\n",
    "            output_embedding = (\n",
    "                full_output.node_embedding\n",
    "                if isinstance(full_output, Output)\n",
    "                else full_output\n",
    "            )\n",
    "            print(candiates_mask, similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f383ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = (\n",
    "                full_output.threshold\n",
    "                if hasattr(full_output, \"threshold\")\n",
    "                else threshold_value\n",
    "            )\n",
    "\n",
    "candidates_mask, similarity_score = threshold_based_candidates(\n",
    "                output, threshold=threshold\n",
    "            )\n",
    "\n",
    "output_embedding = (\n",
    "                full_output.node_embedding\n",
    "                if isinstance(full_output, Output)\n",
    "                else full_output\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff9b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batched_subgraphs.append(batched_subgraphs.x.detach().cpu())\n",
    "all_question_embeddings.append(question_embeddings.detach().cpu())\n",
    "all_candidates_masks.append(candidates_mask.detach().cpu())\n",
    "all_node_maps.extend(node_maps)\n",
    "all_labels.extend(labels)\n",
    "all_output_embeddings.append(output_embedding.detach().cpu())\n",
    "\n",
    "# Only append similarity scores if they are not None\n",
    "if similarity_score is not None:\n",
    "    all_similarity_scores.append(\n",
    "        similarity_score.detach().cpu()\n",
    "    )  # Ensures tensor format\n",
    "else:\n",
    "    print(\"Skipping batch with no similarity scores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12601656",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m candidate_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnonzero(mask, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Where mask is True\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Map subgraph indices to original graph indices using node_map\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m batch_high_similarity_nodes \u001b[38;5;241m=\u001b[39m [node_map[idx\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m candidate_indices]\n\u001b[1;32m      8\u001b[0m high_similarity_nodes\u001b[38;5;241m.\u001b[39mappend(batch_high_similarity_nodes)\n",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m candidate_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnonzero(mask, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Where mask is True\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Map subgraph indices to original graph indices using node_map\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m batch_high_similarity_nodes \u001b[38;5;241m=\u001b[39m [\u001b[43mnode_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m candidate_indices]\n\u001b[1;32m      8\u001b[0m high_similarity_nodes\u001b[38;5;241m.\u001b[39mappend(batch_high_similarity_nodes)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# high_similarity_nodes = []\n",
    "# for batch_idx, (mask, node_map) in enumerate(zip(candidates_mask, node_maps)):\n",
    "#     # Get indices of high-similarity candidates\n",
    "#     candidate_indices = torch.nonzero(mask, as_tuple=True)[0]  # Where mask is True\n",
    "\n",
    "#     # Map subgraph indices to original graph indices using node_map\n",
    "#     batch_high_similarity_nodes = [node_map[idx.item()] for idx in candidate_indices]\n",
    "#     high_similarity_nodes.append(batch_high_similarity_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5032becf",
   "metadata": {},
   "source": [
    "# Test 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "eff3d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_16 = KGQADataset(\n",
    "    path_to_node_embed=config['node_embed'],\n",
    "    path_to_idxes=config['idxes'],\n",
    "    path_to_qa=config['train_qa_data'],\n",
    "    path_to_kb=config['raw_kb'],\n",
    "    from_paths_activate=False,\n",
    "    entity_sbert=False,\n",
    "    k=config['num_hops']\n",
    ")\n",
    "\n",
    "num_relations_16 = train_dataset_16.num_relations # extract the num_relation from the entire graph\n",
    "sub_train_dataset_16 = Subset(train_dataset_16, list(range(config['train']['start_idx'],config['train']['end_idx'])))\n",
    "\n",
    "train_loader_16 = DataLoader(\n",
    "    sub_train_dataset_16,\n",
    "    batch_size=config['train']['batch_size'],\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "10de7c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86605/887694953.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('/hpctmp/e0315913/demo/CS5284_Project/GNN-cluster/src/checkpoints/Test_16/best_model_epoch_32.pth')\n"
     ]
    }
   ],
   "source": [
    "model_test16 = RGCNModel(\n",
    "            node_dim=64,\n",
    "            question_dim=train_dataset_16.q_embeddings.size(-1),\n",
    "            hidden_dim=64,\n",
    "            num_relations=num_relations_16,\n",
    "            output_dim=config['model']['out_channels'],\n",
    "            num_rgcn=config['model']['num_layers'],\n",
    "            reduced_qn_dim=config['model']['reduced_qn_dim'],\n",
    "            reduced_node_dim=config['model']['reduced_node_dim'],\n",
    "            output_embedding=config['model']['output_embedding'],\n",
    "            use_residuals=config['model']['use_residuals']\n",
    "        )\n",
    "\n",
    "checkpoint = torch.load('/hpctmp/e0315913/demo/CS5284_Project/GNN-cluster/src/checkpoints/Test_16/best_model_epoch_32.pth')\n",
    "model_test16.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_test16 = model_test16.to(device)\n",
    "\n",
    "equal_subgraph_weighting = True\n",
    "threshold_value = 0.5\n",
    "hits_at_k = 3\n",
    "\n",
    "save_all_path_test16 = '/hpctmp/e0315913/demo/CS5284_Project/GNN-cluster/data/demo/candidate_metadata_test16.pt'\n",
    "save_emb_path_test16 = '/hpctmp/e0315913/demo/CS5284_Project/GNN-cluster/data/demo/subgraph_qembedding_test16.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4c87727a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/hpctmp/e0315913/demo/CS5284_Project/GNN-cluster/data/demo/candidate_metadata_test16.pt'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_all_path_test16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d1e19be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/hpctmp/e0315913/demo/CS5284_Project/GNN-cluster/data/demo/subgraph_qembedding_test16.pt'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_emb_path_test8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "65cf9e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting subgraph: 100%|██████████| 1/1 [00:00<00:00, 37.00it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_subgraph_qemb(train_loader_16, \n",
    "                     model_test16,\n",
    "                     device, \n",
    "                     threshold_value,\n",
    "                     save_all_path_test16,\n",
    "                     save_emb_path_test16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b549c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
